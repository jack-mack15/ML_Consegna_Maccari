{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ros-guUeWRXe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 19:02:22.613035: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importazione file dataset e creazione di train, test e validation set.\n",
    "Le immagini vengono normalizzate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1708100730220,
     "user": {
      "displayName": "Gianluca Maccari",
      "userId": "05720923772203941125"
     },
     "user_tz": -60
    },
    "id": "7mF1YvipZGAq"
   },
   "outputs": [],
   "source": [
    "x = np.load(\"/home/gianluca/Downloads/pneumonia_images.npy\")\n",
    "y = np.load(\"/home/gianluca/Downloads/pneumonia_labels.npy\")\n",
    "\n",
    "#creazione dei vari dataset\n",
    "seed = 42\n",
    "#separazione train e test set\n",
    "train_images, test_images = train_test_split(x, test_size=0.1, random_state=seed)\n",
    "train_labels, test_labels = train_test_split(y, test_size=0.1, random_state=seed)\n",
    "\n",
    "#creazione validation set e train set\n",
    "x_valid_images = train_images[500:]/255.\n",
    "x_valid_labels = train_labels[500:]\n",
    "x_train_images = train_images[:500]/255.\n",
    "x_train_labels = train_labels[:500]\n",
    "\n",
    "#normalizzazione anche del test set\n",
    "test_images = test_images/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si può osservare come i dati del dataset non presentano errori o immagini compromesse, inoltre si osserva che le immagini possiedono un solo canale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errori nelle label:  0\n",
      "foto compromesse:  0\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#semplice verifica se ci siano dati mancanti o valori differenti da 0 e 1 per le label\n",
    "len_y = len(y)\n",
    "len_image = len(x[0])\n",
    "count_y = 0\n",
    "count_x = 0\n",
    "for i in range(len_y-1):\n",
    "    if y[i] != 0 and y[i] != 1:\n",
    "        count_y += 1\n",
    "    if len(x[i]) != len_image:\n",
    "        count_x += 1\n",
    "print(\"errori nelle label: \",count_y)\n",
    "print(\"foto compromesse: \",count_x)\n",
    "\n",
    "print(x[0].shape)\n",
    "\n",
    "plt.imshow(x[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classe uno ha:  3882  elementi\n",
      "classe zero ha:  1349  elementi\n"
     ]
    }
   ],
   "source": [
    "#verifica sul numero di elementi per classe\n",
    "lab_one = 0\n",
    "lab_zero = 0\n",
    "for i in range(len_y-1):\n",
    "    if y[i] == 0:\n",
    "        lab_zero += 1\n",
    "    else:\n",
    "        lab_one += 1\n",
    "\n",
    "print(\"classe uno ha: \", lab_one,\" elementi\")\n",
    "print(\"classe zero ha: \", lab_zero,\" elementi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1708100730221,
     "user": {
      "displayName": "Gianluca Maccari",
      "userId": "05720923772203941125"
     },
     "user_tz": -60
    },
    "id": "BWpzcRmBky1D"
   },
   "outputs": [],
   "source": [
    "#funzioni utili\n",
    "\n",
    "def printResults(history):\n",
    "  pd.DataFrame(history.history).plot(figsize=(5, 3))\n",
    "  plt.grid(True)\n",
    "  plt.gca().set_ylim(0, 1)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8evNMcPP-2b1"
   },
   "source": [
    "# FNN\n",
    "Si inizia ad affrontare il problema con le reti neurali feedforward. Successivamente si passa alle reti cnn. \n",
    "Le reti cnn hanno delle prestazioni migliori rispetto alle fnn quando si parla di immagini, però è interessante vedere per il task in questione quanto le cnn si comportino meglio rispetto alle fnn, o se le fnn in problemi di binary image classification si comportano bene. \n",
    "La fnn che si ottiene in questa porzione di notebook ha svolto il ruolo principale di indicazione guida sul comportamento delle cnn. Ovvero la famiglia di modelli scelta per affrontare il problema di binary image classification proposto è la famiglia delle reti neurali convoluzionali. Quindi la rete fnn che si ottiene è solo per osservare se il lavoro svolto con le cnn sia corretto.\n",
    "\n",
    "Si esplorano più reti fnn in base al numero di layer e numero di unità per layer.\n",
    "Le reti avranno in comune gli ultimi 3 livelli densi da 32, 16 e 1 livello, mentre variano i primi livelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definisco un modello per testare dei valori differenti per numero di unità e numero layer\n",
    "#i layer finali sono fissi con 32 unità e 16 unità, variano gli upper layer, i primi livelli\n",
    "def build_model(n_layers, units_array):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        model.add(keras.layers.Dense(units_array[i], activation='relu'))\n",
    "\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#funzione utile per addestrare un modello creato con la funziona sopra\n",
    "#l'addestramento viene sempre arrestato da early stopping, quindi il numero di epoche indicato è \n",
    "#sufficiente per l'addestramento completo\n",
    "def train_model(model):\n",
    "    model.fit(x_train_images, x_train_labels, epochs=30,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_valid_images, x_valid_labels),\n",
    "                    callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risultati su test set modello:\n",
      "layers:  4\n",
      "units:  [512, 256, 128, 64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 19:02:23.968188: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 1s 28ms/step - loss: 0.2250 - accuracy: 0.8969\n",
      "loss:  0.22498075664043427\n",
      "accuracy:  0.8969465494155884\n",
      "\n",
      "\n",
      "risultati su test set modello:\n",
      "layers:  3\n",
      "units:  [256, 128, 64]\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1480 - accuracy: 0.9466\n",
      "loss:  0.14804808795452118\n",
      "accuracy:  0.9465649127960205\n",
      "\n",
      "\n",
      "risultati su test set modello:\n",
      "layers:  2\n",
      "units:  [128, 64]\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.1452 - accuracy: 0.9408\n",
      "loss:  0.14517514407634735\n",
      "accuracy:  0.9408397078514099\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 è il numero massimo di upper layer\n",
    "#il vettore numero di unità è [512,256,128,64]\n",
    "#se si volessero modificare i valori da testare: max_layer = len(n_units)\n",
    "n_units = [512,256,128,64]\n",
    "max_layer = 4\n",
    "\n",
    "loss_results = []\n",
    "accuracy_results = []\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for i in range(max_layer-1):\n",
    "    if i == 0:\n",
    "        cur_model = build_model(max_layer,n_units)\n",
    "    else:\n",
    "        cur_model = build_model(max_layer-i,n_units[i:])\n",
    "\n",
    "    model_list.append(cur_model)\n",
    "    print(\"risultati su test set modello:\")\n",
    "    print(\"layers: \",max_layer-i)\n",
    "    print(\"units: \",n_units[i:])\n",
    "    \n",
    "    train_model(cur_model)\n",
    "    cur_loss, cur_acc = cur_model.evaluate(test_images, test_labels)\n",
    "    \n",
    "    print(\"loss: \",cur_loss)\n",
    "    print(\"accuracy: \",cur_acc)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    loss_results.append(cur_loss)\n",
    "    accuracy_results.append(cur_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238,529\n",
      "Trainable params: 238,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1480 - accuracy: 0.9466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14804808795452118, 0.9465649127960205]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prendo il modello con miglior accuracy tra quelli provati\n",
    "n_units = [512,256,128,64]\n",
    "max_layer = 4\n",
    "\n",
    "index = np.argmax(accuracy_results)\n",
    "\n",
    "model = model_list[index]\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo modello è stato utilizzato solo come linea guida per il comportamento dei modelli basati su reti convoluzionali.\n",
    "Inoltre, il training dei tre modelli fnn viene terminato dall'early stopping. Quindi i valori utilizzati come linea guida sono gli stessi utilizzati per scegliere la migliore rete neurale feedforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8evNMcPP-2b1"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1708100752491,
     "user": {
      "displayName": "Gianluca Maccari",
      "userId": "05720923772203941125"
     },
     "user_tz": -60
    },
    "id": "JfEVwYpdxASe"
   },
   "outputs": [],
   "source": [
    "x_tensor_train = x_train_images.reshape(-1,28,28,1)\n",
    "x_tensor_valid = x_valid_images.reshape(-1,28,28,1)\n",
    "test_tensor = test_images.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si utilizza come prima rete cnn la rete LeNet, dove al posto delle funzioni di attivazioni sono state sostituite le sigmoid con le relu. Questa rete si utilizza come base per ottenere le successive reti cnn. Inoltre questa rete viene utilizzata anche per testare la miglior loss function per il task in questione.\n",
    "\n",
    "Come per le reti precedenti, si aggiunge un livello finale per la binary classification.\n",
    "Inoltre si testa quale sia la miglior operazione del livello di pooling tra max pooling e average pooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lenet(pool_type, loss):\n",
    "    \n",
    "    lenet = tf.keras.models.Sequential()\n",
    "    lenet.add(keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "            \n",
    "    match pool_type:\n",
    "        case 1:\n",
    "              lenet.add(keras.layers.AvgPool2D(pool_size=2, strides=2))\n",
    "        case 2:\n",
    "              lenet.add(keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            \n",
    "    lenet.add(keras.layers.Conv2D(filters=16, kernel_size=5, activation='relu'))\n",
    "\n",
    "    match pool_type:\n",
    "        case 1:\n",
    "              lenet.add(keras.layers.AvgPool2D(pool_size=2, strides=2))\n",
    "        case 2:\n",
    "              lenet.add(keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            \n",
    "    lenet.add(keras.layers.Flatten())\n",
    "    lenet.add(keras.layers.Dense(120, activation='relu'))\n",
    "    lenet.add(keras.layers.Dense(84, activation='relu'))\n",
    "    lenet.add(keras.layers.Dense(10, activation='relu'))\n",
    "    lenet.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    lenet.compile(loss=loss,\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "    return lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1526 - accuracy: 0.9370\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1501 - accuracy: 0.9370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1500503569841385, 0.9370229244232178]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet1 = build_lenet(1,\"binary_crossentropy\")\n",
    "lenet2 = build_lenet(2,\"binary_crossentropy\")\n",
    "\n",
    "lenet1.fit(x_tensor_train, x_train_labels, epochs=20, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)])\n",
    "lenet2.fit(x_tensor_train, x_train_labels, epochs=20, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)])\n",
    "\n",
    "lenet1.evaluate(test_tensor, test_labels)\n",
    "lenet2.evaluate(test_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In generale le due operazioni di pooling non hanno prestazioni molto differenti per quanto riguarda l'accuracy sul test set. La loss del test set in più iterazioni sembra risultare più bassa (di poco) con il max pooling.\n",
    "Si sceglie dunque il max pooling come livello di pooling dei successivi modelli.\n",
    "\n",
    "Per la scelta della loss function vengono testate 3 loss function:loss1 = \"binary_crossentropy\", \"mean_squared_error\" e \"binary_focal_crossentropy\". Vengono testate usando LeNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9599\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9504\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.0436 - accuracy: 0.9485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04363809898495674, 0.9484732747077942]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss1 = \"binary_crossentropy\"\n",
    "loss2 = \"mean_squared_error\"\n",
    "loss3 = \"binary_focal_crossentropy\"\n",
    "\n",
    "lenet1 = build_lenet(2,loss1)\n",
    "lenet2 = build_lenet(2,loss2)\n",
    "lenet3 = build_lenet(2,loss3)\n",
    "\n",
    "lenet1.fit(x_tensor_train, x_train_labels, epochs=50, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=7)])\n",
    "lenet2.fit(x_tensor_train, x_train_labels, epochs=50, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=7)])\n",
    "lenet3.fit(x_tensor_train, x_train_labels, epochs=50, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=7)])\n",
    "\n",
    "lenet1.evaluate(test_tensor, test_labels)\n",
    "lenet2.evaluate(test_tensor, test_labels)\n",
    "lenet3.evaluate(test_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se si considera la accuracy sul test set, la miglior loss function è la binary cross entropy. Già da questo primo test si può notare come, considerando il valore di loss per il test set, la miglior loss function sia la binary focal cross entropy.\n",
    "Anche con la rete cnn2 definita più avanti, sono stati ottenuti gli stessi risultati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1109 - accuracy: 0.9714\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0520 - accuracy: 0.9294\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0311 - accuracy: 0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.031137382611632347, 0.9561068415641785]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet1 = build_lenet(2,loss1)\n",
    "lenet2 = build_lenet(2,loss2)\n",
    "lenet3 = build_lenet(2,loss3)\n",
    "\n",
    "lenet1.fit(x_tensor_train, x_train_labels, epochs=50, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7)])\n",
    "lenet2.fit(x_tensor_train, x_train_labels, epochs=50, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7)])\n",
    "lenet3.fit(x_tensor_train, x_train_labels, epochs=50, verbose=0,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7)])\n",
    "\n",
    "lenet1.evaluate(test_tensor, test_labels)\n",
    "lenet2.evaluate(test_tensor, test_labels)\n",
    "lenet3.evaluate(test_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poichè con i seguenti modelli si da più attenzione al valore di accuracy, si sceglie di utilizzare come loss function la binary cross entropy.\n",
    "\n",
    "Nonostante sia stato scelto di concentrarsi sulla validation accuracy, i risultati per la metrica di loss sono comunque sufficienti per giustificare tale scelta.\n",
    "\n",
    "\n",
    "Si estende LeNet con altri 3 livelli convoluzionali e due livelli di max pooling. Inoltre tutti i livelli convoluzionali applicano padding in modo tale che dimensione di input = dimensione output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 4s 204ms/step - loss: 0.1552 - accuracy: 0.7160 - val_loss: 0.1451 - val_accuracy: 0.7407\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 3s 191ms/step - loss: 0.1468 - accuracy: 0.7240 - val_loss: 0.1374 - val_accuracy: 0.7407\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 3s 181ms/step - loss: 0.1316 - accuracy: 0.7240 - val_loss: 0.0985 - val_accuracy: 0.7407\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 3s 183ms/step - loss: 0.1021 - accuracy: 0.8120 - val_loss: 0.1025 - val_accuracy: 0.8258\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 3s 179ms/step - loss: 0.0908 - accuracy: 0.8360 - val_loss: 0.0710 - val_accuracy: 0.9009\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 3s 187ms/step - loss: 0.0765 - accuracy: 0.8640 - val_loss: 0.0638 - val_accuracy: 0.8973\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0712 - accuracy: 0.8980 - val_loss: 0.0567 - val_accuracy: 0.9190\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 3s 192ms/step - loss: 0.0565 - accuracy: 0.9120 - val_loss: 0.0611 - val_accuracy: 0.9085\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0572 - accuracy: 0.9180 - val_loss: 0.0462 - val_accuracy: 0.9354\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0478 - accuracy: 0.9220 - val_loss: 0.0463 - val_accuracy: 0.9249\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 3s 215ms/step - loss: 0.0344 - accuracy: 0.9440 - val_loss: 0.0451 - val_accuracy: 0.9401\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.0314 - accuracy: 0.9500 - val_loss: 0.0408 - val_accuracy: 0.9465\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0380 - accuracy: 0.9520 - val_loss: 0.0458 - val_accuracy: 0.9337\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0278 - accuracy: 0.9620 - val_loss: 0.0353 - val_accuracy: 0.9489\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0205 - accuracy: 0.9720 - val_loss: 0.0415 - val_accuracy: 0.9458\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 3s 221ms/step - loss: 0.0146 - accuracy: 0.9820 - val_loss: 0.0453 - val_accuracy: 0.9472\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0153 - accuracy: 0.9700 - val_loss: 0.0604 - val_accuracy: 0.9411\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0111 - accuracy: 0.9880 - val_loss: 0.0705 - val_accuracy: 0.9301\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 3s 223ms/step - loss: 0.0150 - accuracy: 0.9740 - val_loss: 0.0470 - val_accuracy: 0.9456\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0146 - accuracy: 0.9800 - val_loss: 0.0829 - val_accuracy: 0.9123\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.0182 - accuracy: 0.9740 - val_loss: 0.0451 - val_accuracy: 0.9451\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.0085 - accuracy: 0.9840 - val_loss: 0.0491 - val_accuracy: 0.9499\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.0069 - accuracy: 0.9900 - val_loss: 0.0638 - val_accuracy: 0.9408\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0049 - accuracy: 0.9960 - val_loss: 0.0663 - val_accuracy: 0.9463\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 7.2624e-04 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9463\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 2.0109e-04 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9468\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 1.1065e-04 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9453\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 6.3229e-05 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9463\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 4.9329e-05 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9468\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 4.1089e-05 - accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 0.9463\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 3.5003e-05 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9461\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 3.0083e-05 - accuracy: 1.0000 - val_loss: 0.1084 - val_accuracy: 0.9465\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 2.6341e-05 - accuracy: 1.0000 - val_loss: 0.1099 - val_accuracy: 0.9465\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 2.3725e-05 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9465\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 2.0992e-05 - accuracy: 1.0000 - val_loss: 0.1129 - val_accuracy: 0.9465\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 1.9550e-05 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9463\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 1.7086e-05 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9dc105f90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1 = tf.keras.models.Sequential([\n",
    "            #input 28x28\n",
    "            tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "            #input 28x28\n",
    "            tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "            #input 14x14\n",
    "            tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='relu', padding='same'),\n",
    "            #input 14x14\n",
    "            tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "            #input 7x7\n",
    "            tf.keras.layers.Conv2D(filters=30, kernel_size=5, activation='relu', padding='same'),\n",
    "            #input 7x7\n",
    "            tf.keras.layers.MaxPool2D(pool_size=2, strides=1),\n",
    "            #input 6x6\n",
    "            tf.keras.layers.Conv2D(filters=50, kernel_size=5, activation='relu', padding='same'),\n",
    "            #input 6x6\n",
    "            tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "            #input 3x3\n",
    "            tf.keras.layers.Conv2D(filters=80, kernel_size=3, activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(120, activation='relu'),\n",
    "            tf.keras.layers.Dense(84, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "cnn1.compile(loss=\"binary_focal_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "cnn1.fit(x_tensor_train, x_train_labels, epochs=100,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10708975791931152, 0.9522900581359863]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn1.evaluate(test_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'accuracy di cnn1 in fase di training è molto alta, in alcune epoche tocca anche il valore 0.99, mentre nel testing l'accuracy scende allo 0.95. Anche la loss aumenta tra training e testing, quindi si cerca di diminuire l'overfitting applicando livelli di dropout.\n",
    "Si aumentano il numero di epoche e la patience poichè i livelli di dropout rallentano il training.\n",
    "Sono stati testati i seguenti valori di dropout:0.10, 0.15, 0.20, 0.25, 0.30. I migliori risultati sono stati ottenuti con dropout 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn2_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #input 28x28\n",
    "    model.add(tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "    #input 28x28\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 14x14\n",
    "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='relu', padding='same'))\n",
    "    #input 14x14\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 7x7\n",
    "    model.add(tf.keras.layers.Conv2D(filters=30, kernel_size=5, activation='relu', padding='same'))\n",
    "    #input 7x7\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 6x6\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=5, activation='relu', padding='same'))\n",
    "    #input 6x6\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 3x3\n",
    "    model.add(tf.keras.layers.Conv2D(filters=80, kernel_size=3, activation='relu'))\n",
    "            \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(120, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(84, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.6440 - accuracy: 0.7100 - val_loss: 0.6297 - val_accuracy: 0.7407\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.5919 - accuracy: 0.7240 - val_loss: 0.6155 - val_accuracy: 0.7407\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.5904 - accuracy: 0.7240 - val_loss: 0.5889 - val_accuracy: 0.7407\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.5883 - accuracy: 0.7240 - val_loss: 0.6057 - val_accuracy: 0.7407\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.5867 - accuracy: 0.7240 - val_loss: 0.6160 - val_accuracy: 0.7407\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.5770 - accuracy: 0.7240 - val_loss: 0.5782 - val_accuracy: 0.7407\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.5563 - accuracy: 0.7240 - val_loss: 0.5417 - val_accuracy: 0.7407\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 4s 251ms/step - loss: 0.4990 - accuracy: 0.7240 - val_loss: 0.4396 - val_accuracy: 0.7407\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 3s 210ms/step - loss: 0.5756 - accuracy: 0.7240 - val_loss: 0.4954 - val_accuracy: 0.7407\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.4941 - accuracy: 0.7240 - val_loss: 0.4359 - val_accuracy: 0.7407\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.4469 - accuracy: 0.7240 - val_loss: 0.4428 - val_accuracy: 0.7407\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.4411 - accuracy: 0.7800 - val_loss: 0.4041 - val_accuracy: 0.8909\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.3501 - accuracy: 0.8400 - val_loss: 0.3373 - val_accuracy: 0.8472\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 0.3039 - accuracy: 0.8720 - val_loss: 0.2318 - val_accuracy: 0.9104\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.3409 - accuracy: 0.8440 - val_loss: 0.3101 - val_accuracy: 0.8764\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.3163 - accuracy: 0.8700 - val_loss: 0.3814 - val_accuracy: 0.8527\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.3066 - accuracy: 0.8720 - val_loss: 0.3016 - val_accuracy: 0.8933\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.2740 - accuracy: 0.8840 - val_loss: 0.2031 - val_accuracy: 0.9256\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.3671 - accuracy: 0.8300 - val_loss: 0.3630 - val_accuracy: 0.9149\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.2816 - accuracy: 0.8780 - val_loss: 0.1991 - val_accuracy: 0.9259\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.2284 - accuracy: 0.9120 - val_loss: 0.2485 - val_accuracy: 0.9133\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.2241 - accuracy: 0.9000 - val_loss: 0.2282 - val_accuracy: 0.9163\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.2215 - accuracy: 0.9060 - val_loss: 0.2679 - val_accuracy: 0.8954\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.2276 - accuracy: 0.9020 - val_loss: 0.2071 - val_accuracy: 0.9237\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.2006 - accuracy: 0.9180 - val_loss: 0.1650 - val_accuracy: 0.9365\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.2387 - accuracy: 0.8980 - val_loss: 0.1740 - val_accuracy: 0.9327\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.2038 - accuracy: 0.9260 - val_loss: 0.2002 - val_accuracy: 0.9356\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.1917 - accuracy: 0.9240 - val_loss: 0.1614 - val_accuracy: 0.9427\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.1788 - accuracy: 0.9320 - val_loss: 0.1426 - val_accuracy: 0.9468\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.1798 - accuracy: 0.9200 - val_loss: 0.1804 - val_accuracy: 0.9275\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.1878 - accuracy: 0.9120 - val_loss: 0.1958 - val_accuracy: 0.9149\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.1986 - accuracy: 0.9100 - val_loss: 0.2132 - val_accuracy: 0.9095\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.2457 - accuracy: 0.8900 - val_loss: 0.1927 - val_accuracy: 0.9327\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.2195 - accuracy: 0.9100 - val_loss: 0.1688 - val_accuracy: 0.9368\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1962 - accuracy: 0.9200 - val_loss: 0.1404 - val_accuracy: 0.9468\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.1753 - accuracy: 0.9320 - val_loss: 0.1359 - val_accuracy: 0.9470\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.1869 - accuracy: 0.9240 - val_loss: 0.1488 - val_accuracy: 0.9482\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.1572 - accuracy: 0.9400 - val_loss: 0.1286 - val_accuracy: 0.9489\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.2188 - accuracy: 0.9120 - val_loss: 0.1650 - val_accuracy: 0.9434\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 3s 208ms/step - loss: 0.1650 - accuracy: 0.9380 - val_loss: 0.1234 - val_accuracy: 0.9522\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.1553 - accuracy: 0.9380 - val_loss: 0.1358 - val_accuracy: 0.9468\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.1451 - accuracy: 0.9480 - val_loss: 0.1228 - val_accuracy: 0.9525\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.1202 - accuracy: 0.9560 - val_loss: 0.1258 - val_accuracy: 0.9506\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.1188 - accuracy: 0.9600 - val_loss: 0.1496 - val_accuracy: 0.9404\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.1319 - accuracy: 0.9460 - val_loss: 0.1756 - val_accuracy: 0.9325\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 3s 215ms/step - loss: 0.1456 - accuracy: 0.9500 - val_loss: 0.1432 - val_accuracy: 0.9413\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.1389 - accuracy: 0.9520 - val_loss: 0.1169 - val_accuracy: 0.9548\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.1287 - accuracy: 0.9400 - val_loss: 0.1116 - val_accuracy: 0.9546\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.1567 - accuracy: 0.9320 - val_loss: 0.1152 - val_accuracy: 0.9544\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.1364 - accuracy: 0.9480 - val_loss: 0.1335 - val_accuracy: 0.9489\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.1499 - accuracy: 0.9460 - val_loss: 0.1126 - val_accuracy: 0.9577\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.1301 - accuracy: 0.9540 - val_loss: 0.1739 - val_accuracy: 0.9306\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1519 - accuracy: 0.9340 - val_loss: 0.1453 - val_accuracy: 0.9385\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.1494 - accuracy: 0.9480 - val_loss: 0.1149 - val_accuracy: 0.9546\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.1166 - accuracy: 0.9520 - val_loss: 0.1124 - val_accuracy: 0.9579\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.1453 - accuracy: 0.9540 - val_loss: 0.1101 - val_accuracy: 0.9577\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.1252 - accuracy: 0.9520 - val_loss: 0.1243 - val_accuracy: 0.9506\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.1080 - accuracy: 0.9500 - val_loss: 0.1490 - val_accuracy: 0.9470\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.1017 - accuracy: 0.9600 - val_loss: 0.1198 - val_accuracy: 0.9534\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.1001 - accuracy: 0.9520 - val_loss: 0.1257 - val_accuracy: 0.9513\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.1160 - accuracy: 0.9440 - val_loss: 0.1157 - val_accuracy: 0.9532\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.1047 - accuracy: 0.9500 - val_loss: 0.1293 - val_accuracy: 0.9513\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 4s 258ms/step - loss: 0.0903 - accuracy: 0.9720 - val_loss: 0.1502 - val_accuracy: 0.9389\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 3s 221ms/step - loss: 0.1054 - accuracy: 0.9600 - val_loss: 0.1276 - val_accuracy: 0.9508\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 3s 215ms/step - loss: 0.0890 - accuracy: 0.9660 - val_loss: 0.1342 - val_accuracy: 0.9487\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0874 - accuracy: 0.9640 - val_loss: 0.1100 - val_accuracy: 0.9598\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 0.1035 - accuracy: 0.9560 - val_loss: 0.1563 - val_accuracy: 0.9503\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1029 - accuracy: 0.9620 - val_loss: 0.1129 - val_accuracy: 0.9575\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0635 - accuracy: 0.9680 - val_loss: 0.1772 - val_accuracy: 0.9468\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0730 - accuracy: 0.9720 - val_loss: 0.1395 - val_accuracy: 0.9539\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0973 - accuracy: 0.9700 - val_loss: 0.1273 - val_accuracy: 0.9487\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0829 - accuracy: 0.9620 - val_loss: 0.1283 - val_accuracy: 0.9539\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.1177 - accuracy: 0.9440 - val_loss: 0.1162 - val_accuracy: 0.9544\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1001 - accuracy: 0.9620 - val_loss: 0.1226 - val_accuracy: 0.9556\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.1228 - accuracy: 0.9400 - val_loss: 0.1155 - val_accuracy: 0.9563\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.1304 - accuracy: 0.9420 - val_loss: 0.1525 - val_accuracy: 0.9361\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.1250 - accuracy: 0.9560 - val_loss: 0.1830 - val_accuracy: 0.9325\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0890 - accuracy: 0.9640 - val_loss: 0.1324 - val_accuracy: 0.9513\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.0845 - accuracy: 0.9660 - val_loss: 0.1084 - val_accuracy: 0.9603\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0726 - accuracy: 0.9740 - val_loss: 0.1263 - val_accuracy: 0.9544\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0964 - accuracy: 0.9720 - val_loss: 0.1587 - val_accuracy: 0.9394\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.1274 - accuracy: 0.9560 - val_loss: 0.1183 - val_accuracy: 0.9541\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.0657 - accuracy: 0.9780 - val_loss: 0.1147 - val_accuracy: 0.9587\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0967 - accuracy: 0.9640 - val_loss: 0.1365 - val_accuracy: 0.9496\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1050 - accuracy: 0.9540 - val_loss: 0.1200 - val_accuracy: 0.9589\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.0887 - accuracy: 0.9580 - val_loss: 0.1269 - val_accuracy: 0.9539\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.1181 - accuracy: 0.9560 - val_loss: 0.1223 - val_accuracy: 0.9527\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.1124 - accuracy: 0.9580 - val_loss: 0.1081 - val_accuracy: 0.9570\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.1080 - accuracy: 0.9680 - val_loss: 0.1257 - val_accuracy: 0.9541\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0866 - accuracy: 0.9720 - val_loss: 0.1271 - val_accuracy: 0.9525\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.0731 - accuracy: 0.9680 - val_loss: 0.1071 - val_accuracy: 0.9620\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0661 - accuracy: 0.9760 - val_loss: 0.1143 - val_accuracy: 0.9584\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.0671 - accuracy: 0.9720 - val_loss: 0.1183 - val_accuracy: 0.9577\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.1143 - val_accuracy: 0.9591\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0629 - accuracy: 0.9740 - val_loss: 0.1223 - val_accuracy: 0.9629\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0722 - accuracy: 0.9740 - val_loss: 0.1161 - val_accuracy: 0.9575\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0601 - accuracy: 0.9780 - val_loss: 0.1269 - val_accuracy: 0.9532\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0844 - accuracy: 0.9640 - val_loss: 0.1523 - val_accuracy: 0.9501\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0857 - accuracy: 0.9600 - val_loss: 0.1231 - val_accuracy: 0.9553\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.1022 - accuracy: 0.9640 - val_loss: 0.1227 - val_accuracy: 0.9553\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.0949 - accuracy: 0.9640 - val_loss: 0.1998 - val_accuracy: 0.9385\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.0554 - accuracy: 0.9780 - val_loss: 0.1458 - val_accuracy: 0.9546\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1093 - accuracy: 0.9540 - val_loss: 0.1109 - val_accuracy: 0.9584\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0790 - accuracy: 0.9660 - val_loss: 0.1454 - val_accuracy: 0.9496\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0697 - accuracy: 0.9720 - val_loss: 0.1123 - val_accuracy: 0.9613\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 0.0570 - accuracy: 0.9800 - val_loss: 0.1245 - val_accuracy: 0.9591\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0310 - accuracy: 0.9940 - val_loss: 0.1396 - val_accuracy: 0.9563\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0716 - accuracy: 0.9700 - val_loss: 0.1305 - val_accuracy: 0.9560\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 237ms/step - loss: 0.0635 - accuracy: 0.9800 - val_loss: 0.1189 - val_accuracy: 0.9548\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0569 - accuracy: 0.9760 - val_loss: 0.1595 - val_accuracy: 0.9444\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0679 - accuracy: 0.9700 - val_loss: 0.1247 - val_accuracy: 0.9567\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0647 - accuracy: 0.9720 - val_loss: 0.1854 - val_accuracy: 0.9532\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0796 - accuracy: 0.9680 - val_loss: 0.1283 - val_accuracy: 0.9560\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0714 - accuracy: 0.9740 - val_loss: 0.1215 - val_accuracy: 0.9558\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0603 - accuracy: 0.9700 - val_loss: 0.1245 - val_accuracy: 0.9610\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0505 - accuracy: 0.9760 - val_loss: 0.1672 - val_accuracy: 0.9487\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 3s 222ms/step - loss: 0.0599 - accuracy: 0.9740 - val_loss: 0.1210 - val_accuracy: 0.9572\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0212 - accuracy: 0.9940 - val_loss: 0.1796 - val_accuracy: 0.9532\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0884 - accuracy: 0.9720 - val_loss: 0.1217 - val_accuracy: 0.9622\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0684 - accuracy: 0.9780 - val_loss: 0.1359 - val_accuracy: 0.9560\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0587 - accuracy: 0.9760 - val_loss: 0.1869 - val_accuracy: 0.9396\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.1050 - accuracy: 0.9660 - val_loss: 0.1378 - val_accuracy: 0.9522\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0695 - accuracy: 0.9640 - val_loss: 0.1102 - val_accuracy: 0.9594\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0484 - accuracy: 0.9780 - val_loss: 0.1297 - val_accuracy: 0.9577\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.0828 - accuracy: 0.9780 - val_loss: 0.1343 - val_accuracy: 0.9518\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.0555 - accuracy: 0.9760 - val_loss: 0.1197 - val_accuracy: 0.9577\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0493 - accuracy: 0.9800 - val_loss: 0.1277 - val_accuracy: 0.9548\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0779 - accuracy: 0.9720 - val_loss: 0.1457 - val_accuracy: 0.9503\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0664 - accuracy: 0.9720 - val_loss: 0.1135 - val_accuracy: 0.9594\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.0854 - accuracy: 0.9700 - val_loss: 0.1314 - val_accuracy: 0.9584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8eba95360>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2 = create_cnn2_model()\n",
    "\n",
    "cnn2.fit(x_tensor_train, x_train_labels, epochs=200,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=35)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08162851631641388, 0.9713740348815918]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2.evaluate(test_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come detto sopra, sono state testate le 3 loss function anche con la rete cnn2 e i risultati migliori sono i seguenti. \n",
    "\n",
    "Binary cross entropy restituisce come migliori valori:\n",
    "loss: 0.0328 - accuracy: 0.9752.\n",
    "\n",
    "Mean squared error loss restituisce i seguenti valori:\n",
    "loss: 0.0329 - accuracy: 0.9523.\n",
    "\n",
    "Binary focal cross entropy restituisce i seguenti valori:\n",
    "loss: 0.0398 - accuracy: 0.9523.\n",
    "\n",
    "(Non vi sono celle di codice riguardo questo controllo perchè occorre cambiare la loss function del modello cnn2. Inoltre gli output del training di cnn2 con le differenti loss function non sono stati riportati nel notebook per alleggerire il carico visivo di questo file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultima modifica che si effettua a cnn2 è aumentare il numero di livelli dense e il numero di unità di tali livelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn3_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    #input 28x28\n",
    "    model.add(tf.keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "    #input 28x28\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 14x14\n",
    "    model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=5, activation='relu', padding='same'))\n",
    "    #input 14x14\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 7x7\n",
    "    model.add(tf.keras.layers.Conv2D(filters=30, kernel_size=5, activation='relu', padding='same'))\n",
    "    #input 7x7\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=1))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 6x6\n",
    "    model.add(tf.keras.layers.Conv2D(filters=50, kernel_size=5, activation='relu', padding='same'))\n",
    "    #input 6x6\n",
    "    model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    #input 3x3\n",
    "    model.add(tf.keras.layers.Conv2D(filters=80, kernel_size=3, activation='relu'))\n",
    "            \n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "16/16 [==============================] - 5s 240ms/step - loss: 0.6292 - accuracy: 0.7200 - val_loss: 0.5832 - val_accuracy: 0.7407\n",
      "Epoch 2/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.6086 - accuracy: 0.7240 - val_loss: 0.5965 - val_accuracy: 0.7407\n",
      "Epoch 3/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.5870 - accuracy: 0.7240 - val_loss: 0.5674 - val_accuracy: 0.7407\n",
      "Epoch 4/200\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.5831 - accuracy: 0.7240 - val_loss: 0.5782 - val_accuracy: 0.7407\n",
      "Epoch 5/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.5458 - accuracy: 0.7240 - val_loss: 0.5010 - val_accuracy: 0.7407\n",
      "Epoch 6/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.5153 - accuracy: 0.7240 - val_loss: 0.4253 - val_accuracy: 0.7407\n",
      "Epoch 7/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.4646 - accuracy: 0.7240 - val_loss: 0.3949 - val_accuracy: 0.7407\n",
      "Epoch 8/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.3894 - accuracy: 0.7240 - val_loss: 0.3734 - val_accuracy: 0.8624\n",
      "Epoch 9/200\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.4160 - accuracy: 0.7980 - val_loss: 0.3972 - val_accuracy: 0.8389\n",
      "Epoch 10/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.3652 - accuracy: 0.8360 - val_loss: 0.4092 - val_accuracy: 0.7854\n",
      "Epoch 11/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.3918 - accuracy: 0.8120 - val_loss: 0.4390 - val_accuracy: 0.8636\n",
      "Epoch 12/200\n",
      "16/16 [==============================] - 3s 210ms/step - loss: 0.3307 - accuracy: 0.8640 - val_loss: 0.2504 - val_accuracy: 0.9111\n",
      "Epoch 13/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.3720 - accuracy: 0.8380 - val_loss: 0.4357 - val_accuracy: 0.8161\n",
      "Epoch 14/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.3288 - accuracy: 0.8600 - val_loss: 0.2625 - val_accuracy: 0.9211\n",
      "Epoch 15/200\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 0.2910 - accuracy: 0.8600 - val_loss: 0.2340 - val_accuracy: 0.9156\n",
      "Epoch 16/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.2505 - accuracy: 0.9020 - val_loss: 0.2465 - val_accuracy: 0.8954\n",
      "Epoch 17/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.2383 - accuracy: 0.9040 - val_loss: 0.2379 - val_accuracy: 0.9083\n",
      "Epoch 18/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.2130 - accuracy: 0.9180 - val_loss: 0.2135 - val_accuracy: 0.9087\n",
      "Epoch 19/200\n",
      "16/16 [==============================] - 4s 219ms/step - loss: 0.2646 - accuracy: 0.8940 - val_loss: 0.2453 - val_accuracy: 0.9085\n",
      "Epoch 20/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.2067 - accuracy: 0.9140 - val_loss: 0.1702 - val_accuracy: 0.9327\n",
      "Epoch 21/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.1940 - accuracy: 0.9260 - val_loss: 0.2788 - val_accuracy: 0.8698\n",
      "Epoch 22/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.2435 - accuracy: 0.8940 - val_loss: 0.1876 - val_accuracy: 0.9358\n",
      "Epoch 23/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1768 - accuracy: 0.9340 - val_loss: 0.1944 - val_accuracy: 0.9237\n",
      "Epoch 24/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.1762 - accuracy: 0.9200 - val_loss: 0.1728 - val_accuracy: 0.9339\n",
      "Epoch 25/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.1765 - accuracy: 0.9220 - val_loss: 0.1451 - val_accuracy: 0.9442\n",
      "Epoch 26/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1753 - accuracy: 0.9220 - val_loss: 0.1683 - val_accuracy: 0.9337\n",
      "Epoch 27/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.1753 - accuracy: 0.9340 - val_loss: 0.1930 - val_accuracy: 0.9232\n",
      "Epoch 28/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.1503 - accuracy: 0.9440 - val_loss: 0.1467 - val_accuracy: 0.9406\n",
      "Epoch 29/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1583 - accuracy: 0.9280 - val_loss: 0.2106 - val_accuracy: 0.9137\n",
      "Epoch 30/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.1411 - accuracy: 0.9460 - val_loss: 0.1476 - val_accuracy: 0.9406\n",
      "Epoch 31/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.1633 - accuracy: 0.9380 - val_loss: 0.1317 - val_accuracy: 0.9529\n",
      "Epoch 32/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1260 - accuracy: 0.9520 - val_loss: 0.2602 - val_accuracy: 0.9085\n",
      "Epoch 33/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.2123 - accuracy: 0.9320 - val_loss: 0.1570 - val_accuracy: 0.9453\n",
      "Epoch 34/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.1555 - accuracy: 0.9480 - val_loss: 0.1542 - val_accuracy: 0.9373\n",
      "Epoch 35/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.1421 - accuracy: 0.9400 - val_loss: 0.1350 - val_accuracy: 0.9427\n",
      "Epoch 36/200\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 0.1420 - accuracy: 0.9380 - val_loss: 0.1590 - val_accuracy: 0.9263\n",
      "Epoch 37/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1675 - accuracy: 0.9320 - val_loss: 0.1304 - val_accuracy: 0.9480\n",
      "Epoch 38/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1326 - accuracy: 0.9400 - val_loss: 0.1680 - val_accuracy: 0.9356\n",
      "Epoch 39/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.1773 - accuracy: 0.9360 - val_loss: 0.1557 - val_accuracy: 0.9453\n",
      "Epoch 40/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.1236 - accuracy: 0.9600 - val_loss: 0.1229 - val_accuracy: 0.9513\n",
      "Epoch 41/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1583 - accuracy: 0.9380 - val_loss: 0.1378 - val_accuracy: 0.9432\n",
      "Epoch 42/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.1148 - accuracy: 0.9520 - val_loss: 0.1251 - val_accuracy: 0.9503\n",
      "Epoch 43/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.1146 - accuracy: 0.9500 - val_loss: 0.1713 - val_accuracy: 0.9337\n",
      "Epoch 44/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.1218 - accuracy: 0.9500 - val_loss: 0.1196 - val_accuracy: 0.9522\n",
      "Epoch 45/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0910 - accuracy: 0.9660 - val_loss: 0.1612 - val_accuracy: 0.9365\n",
      "Epoch 46/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.1533 - accuracy: 0.9340 - val_loss: 0.1410 - val_accuracy: 0.9361\n",
      "Epoch 47/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.1300 - accuracy: 0.9480 - val_loss: 0.1146 - val_accuracy: 0.9579\n",
      "Epoch 48/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.1260 - accuracy: 0.9440 - val_loss: 0.1261 - val_accuracy: 0.9439\n",
      "Epoch 49/200\n",
      "16/16 [==============================] - 4s 246ms/step - loss: 0.1261 - accuracy: 0.9460 - val_loss: 0.1281 - val_accuracy: 0.9442\n",
      "Epoch 50/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1344 - accuracy: 0.9420 - val_loss: 0.1644 - val_accuracy: 0.9292\n",
      "Epoch 51/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.1263 - accuracy: 0.9520 - val_loss: 0.1306 - val_accuracy: 0.9458\n",
      "Epoch 52/200\n",
      "16/16 [==============================] - 3s 222ms/step - loss: 0.1276 - accuracy: 0.9400 - val_loss: 0.1729 - val_accuracy: 0.9382\n",
      "Epoch 53/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.1273 - accuracy: 0.9520 - val_loss: 0.1197 - val_accuracy: 0.9544\n",
      "Epoch 54/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.1008 - accuracy: 0.9580 - val_loss: 0.1288 - val_accuracy: 0.9556\n",
      "Epoch 55/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.1290 - accuracy: 0.9340 - val_loss: 0.1582 - val_accuracy: 0.9292\n",
      "Epoch 56/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.1039 - accuracy: 0.9540 - val_loss: 0.1200 - val_accuracy: 0.9518\n",
      "Epoch 57/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.1102 - accuracy: 0.9580 - val_loss: 0.1349 - val_accuracy: 0.9418\n",
      "Epoch 58/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.1125 - accuracy: 0.9620 - val_loss: 0.1192 - val_accuracy: 0.9532\n",
      "Epoch 59/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.1267 - accuracy: 0.9500 - val_loss: 0.1860 - val_accuracy: 0.9389\n",
      "Epoch 60/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.1314 - accuracy: 0.9540 - val_loss: 0.1069 - val_accuracy: 0.9591\n",
      "Epoch 61/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.1000 - accuracy: 0.9580 - val_loss: 0.1197 - val_accuracy: 0.9553\n",
      "Epoch 62/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.0770 - accuracy: 0.9680 - val_loss: 0.1144 - val_accuracy: 0.9575\n",
      "Epoch 63/200\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.1310 - accuracy: 0.9600 - val_loss: 0.1597 - val_accuracy: 0.9363\n",
      "Epoch 64/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0868 - accuracy: 0.9740 - val_loss: 0.1189 - val_accuracy: 0.9558\n",
      "Epoch 65/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.1351 - accuracy: 0.9540 - val_loss: 0.1215 - val_accuracy: 0.9520\n",
      "Epoch 66/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.1135 - accuracy: 0.9600 - val_loss: 0.1080 - val_accuracy: 0.9594\n",
      "Epoch 67/200\n",
      "16/16 [==============================] - 3s 211ms/step - loss: 0.1028 - accuracy: 0.9520 - val_loss: 0.1095 - val_accuracy: 0.9584\n",
      "Epoch 68/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.0877 - accuracy: 0.9660 - val_loss: 0.1540 - val_accuracy: 0.9434\n",
      "Epoch 69/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0998 - accuracy: 0.9680 - val_loss: 0.1395 - val_accuracy: 0.9470\n",
      "Epoch 70/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.1037 - accuracy: 0.9580 - val_loss: 0.1168 - val_accuracy: 0.9544\n",
      "Epoch 71/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0980 - accuracy: 0.9640 - val_loss: 0.1140 - val_accuracy: 0.9553\n",
      "Epoch 72/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0842 - accuracy: 0.9660 - val_loss: 0.1067 - val_accuracy: 0.9591\n",
      "Epoch 73/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0916 - accuracy: 0.9620 - val_loss: 0.1352 - val_accuracy: 0.9522\n",
      "Epoch 74/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0754 - accuracy: 0.9640 - val_loss: 0.1163 - val_accuracy: 0.9537\n",
      "Epoch 75/200\n",
      "16/16 [==============================] - 4s 247ms/step - loss: 0.0755 - accuracy: 0.9780 - val_loss: 0.1261 - val_accuracy: 0.9515\n",
      "Epoch 76/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.0812 - accuracy: 0.9640 - val_loss: 0.1104 - val_accuracy: 0.9572\n",
      "Epoch 77/200\n",
      "16/16 [==============================] - 3s 219ms/step - loss: 0.0471 - accuracy: 0.9780 - val_loss: 0.2062 - val_accuracy: 0.9415\n",
      "Epoch 78/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1340 - accuracy: 0.9660 - val_loss: 0.1488 - val_accuracy: 0.9423\n",
      "Epoch 79/200\n",
      "16/16 [==============================] - 3s 221ms/step - loss: 0.0974 - accuracy: 0.9580 - val_loss: 0.1296 - val_accuracy: 0.9532\n",
      "Epoch 80/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0790 - accuracy: 0.9720 - val_loss: 0.1118 - val_accuracy: 0.9577\n",
      "Epoch 81/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0630 - accuracy: 0.9760 - val_loss: 0.1129 - val_accuracy: 0.9598\n",
      "Epoch 82/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0734 - accuracy: 0.9700 - val_loss: 0.1280 - val_accuracy: 0.9551\n",
      "Epoch 83/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0956 - accuracy: 0.9660 - val_loss: 0.1480 - val_accuracy: 0.9453\n",
      "Epoch 84/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.1202 - accuracy: 0.9580 - val_loss: 0.1603 - val_accuracy: 0.9327\n",
      "Epoch 85/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0783 - accuracy: 0.9700 - val_loss: 0.1253 - val_accuracy: 0.9518\n",
      "Epoch 86/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0811 - accuracy: 0.9660 - val_loss: 0.1202 - val_accuracy: 0.9565\n",
      "Epoch 87/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0685 - accuracy: 0.9660 - val_loss: 0.1229 - val_accuracy: 0.9570\n",
      "Epoch 88/200\n",
      "16/16 [==============================] - 3s 211ms/step - loss: 0.0879 - accuracy: 0.9660 - val_loss: 0.1434 - val_accuracy: 0.9541\n",
      "Epoch 89/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0963 - accuracy: 0.9680 - val_loss: 0.1230 - val_accuracy: 0.9529\n",
      "Epoch 90/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.0719 - accuracy: 0.9720 - val_loss: 0.1151 - val_accuracy: 0.9603\n",
      "Epoch 91/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0819 - accuracy: 0.9700 - val_loss: 0.1186 - val_accuracy: 0.9570\n",
      "Epoch 92/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0716 - accuracy: 0.9760 - val_loss: 0.1165 - val_accuracy: 0.9544\n",
      "Epoch 93/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.1131 - accuracy: 0.9540 - val_loss: 0.1009 - val_accuracy: 0.9610\n",
      "Epoch 94/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0671 - accuracy: 0.9700 - val_loss: 0.1265 - val_accuracy: 0.9553\n",
      "Epoch 95/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0378 - accuracy: 0.9840 - val_loss: 0.1404 - val_accuracy: 0.9539\n",
      "Epoch 96/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0306 - accuracy: 0.9860 - val_loss: 0.2521 - val_accuracy: 0.9244\n",
      "Epoch 97/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0869 - accuracy: 0.9660 - val_loss: 0.1349 - val_accuracy: 0.9508\n",
      "Epoch 98/200\n",
      "16/16 [==============================] - 3s 218ms/step - loss: 0.0842 - accuracy: 0.9660 - val_loss: 0.1518 - val_accuracy: 0.9396\n",
      "Epoch 99/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.0940 - accuracy: 0.9720 - val_loss: 0.1263 - val_accuracy: 0.9567\n",
      "Epoch 100/200\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 0.0686 - accuracy: 0.9720 - val_loss: 0.1290 - val_accuracy: 0.9556\n",
      "Epoch 101/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.1076 - accuracy: 0.9640 - val_loss: 0.1149 - val_accuracy: 0.9534\n",
      "Epoch 102/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0657 - accuracy: 0.9680 - val_loss: 0.1823 - val_accuracy: 0.9415\n",
      "Epoch 103/200\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.0937 - accuracy: 0.9640 - val_loss: 0.1278 - val_accuracy: 0.9534\n",
      "Epoch 104/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0754 - accuracy: 0.9640 - val_loss: 0.1158 - val_accuracy: 0.9563\n",
      "Epoch 105/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.0544 - accuracy: 0.9800 - val_loss: 0.1192 - val_accuracy: 0.9615\n",
      "Epoch 106/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.1134 - accuracy: 0.9540 - val_loss: 0.1930 - val_accuracy: 0.9240\n",
      "Epoch 107/200\n",
      "16/16 [==============================] - 3s 210ms/step - loss: 0.0699 - accuracy: 0.9760 - val_loss: 0.1266 - val_accuracy: 0.9570\n",
      "Epoch 108/200\n",
      "16/16 [==============================] - 4s 218ms/step - loss: 0.0972 - accuracy: 0.9660 - val_loss: 0.1030 - val_accuracy: 0.9610\n",
      "Epoch 109/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0822 - accuracy: 0.9700 - val_loss: 0.0991 - val_accuracy: 0.9608\n",
      "Epoch 110/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0771 - accuracy: 0.9600 - val_loss: 0.1222 - val_accuracy: 0.9525\n",
      "Epoch 111/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.0580 - accuracy: 0.9760 - val_loss: 0.1266 - val_accuracy: 0.9572\n",
      "Epoch 112/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0579 - accuracy: 0.9800 - val_loss: 0.1153 - val_accuracy: 0.9598\n",
      "Epoch 113/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0440 - accuracy: 0.9840 - val_loss: 0.1244 - val_accuracy: 0.9570\n",
      "Epoch 114/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0661 - accuracy: 0.9720 - val_loss: 0.1349 - val_accuracy: 0.9584\n",
      "Epoch 115/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0590 - accuracy: 0.9820 - val_loss: 0.1286 - val_accuracy: 0.9534\n",
      "Epoch 116/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.0428 - accuracy: 0.9860 - val_loss: 0.1163 - val_accuracy: 0.9620\n",
      "Epoch 117/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0600 - accuracy: 0.9720 - val_loss: 0.1946 - val_accuracy: 0.9413\n",
      "Epoch 118/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0727 - accuracy: 0.9760 - val_loss: 0.1093 - val_accuracy: 0.9608\n",
      "Epoch 119/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.0693 - accuracy: 0.9780 - val_loss: 0.1111 - val_accuracy: 0.9598\n",
      "Epoch 120/200\n",
      "16/16 [==============================] - 3s 217ms/step - loss: 0.0620 - accuracy: 0.9760 - val_loss: 0.1210 - val_accuracy: 0.9591\n",
      "Epoch 121/200\n",
      "16/16 [==============================] - 4s 269ms/step - loss: 0.0409 - accuracy: 0.9880 - val_loss: 0.1162 - val_accuracy: 0.9596\n",
      "Epoch 122/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0291 - accuracy: 0.9920 - val_loss: 0.1498 - val_accuracy: 0.9603\n",
      "Epoch 123/200\n",
      "16/16 [==============================] - 4s 243ms/step - loss: 0.0763 - accuracy: 0.9740 - val_loss: 0.2137 - val_accuracy: 0.9323\n",
      "Epoch 124/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.0945 - accuracy: 0.9740 - val_loss: 0.1124 - val_accuracy: 0.9544\n",
      "Epoch 125/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0503 - accuracy: 0.9820 - val_loss: 0.1259 - val_accuracy: 0.9572\n",
      "Epoch 126/200\n",
      "16/16 [==============================] - 3s 203ms/step - loss: 0.0678 - accuracy: 0.9720 - val_loss: 0.1057 - val_accuracy: 0.9625\n",
      "Epoch 127/200\n",
      "16/16 [==============================] - 3s 216ms/step - loss: 0.0884 - accuracy: 0.9660 - val_loss: 0.1130 - val_accuracy: 0.9587\n",
      "Epoch 128/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0539 - accuracy: 0.9780 - val_loss: 0.1491 - val_accuracy: 0.9515\n",
      "Epoch 129/200\n",
      "16/16 [==============================] - 4s 226ms/step - loss: 0.0699 - accuracy: 0.9760 - val_loss: 0.1122 - val_accuracy: 0.9596\n",
      "Epoch 130/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0534 - accuracy: 0.9760 - val_loss: 0.1251 - val_accuracy: 0.9601\n",
      "Epoch 131/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0310 - accuracy: 0.9900 - val_loss: 0.1310 - val_accuracy: 0.9629\n",
      "Epoch 132/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0584 - accuracy: 0.9840 - val_loss: 0.1313 - val_accuracy: 0.9589\n",
      "Epoch 133/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0637 - accuracy: 0.9700 - val_loss: 0.1258 - val_accuracy: 0.9499\n",
      "Epoch 134/200\n",
      "16/16 [==============================] - 3s 206ms/step - loss: 0.0456 - accuracy: 0.9820 - val_loss: 0.1223 - val_accuracy: 0.9563\n",
      "Epoch 135/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0532 - accuracy: 0.9720 - val_loss: 0.1236 - val_accuracy: 0.9587\n",
      "Epoch 136/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.0564 - accuracy: 0.9760 - val_loss: 0.1195 - val_accuracy: 0.9629\n",
      "Epoch 137/200\n",
      "16/16 [==============================] - 3s 215ms/step - loss: 0.0418 - accuracy: 0.9800 - val_loss: 0.1726 - val_accuracy: 0.9565\n",
      "Epoch 138/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0503 - accuracy: 0.9800 - val_loss: 0.1196 - val_accuracy: 0.9627\n",
      "Epoch 139/200\n",
      "16/16 [==============================] - 4s 253ms/step - loss: 0.0583 - accuracy: 0.9780 - val_loss: 0.1077 - val_accuracy: 0.9610\n",
      "Epoch 140/200\n",
      "16/16 [==============================] - 4s 225ms/step - loss: 0.0421 - accuracy: 0.9820 - val_loss: 0.1630 - val_accuracy: 0.9501\n",
      "Epoch 141/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0487 - accuracy: 0.9860 - val_loss: 0.1118 - val_accuracy: 0.9615\n",
      "Epoch 142/200\n",
      "16/16 [==============================] - 4s 249ms/step - loss: 0.0717 - accuracy: 0.9840 - val_loss: 0.1653 - val_accuracy: 0.9396\n",
      "Epoch 143/200\n",
      "16/16 [==============================] - 4s 238ms/step - loss: 0.0636 - accuracy: 0.9720 - val_loss: 0.1689 - val_accuracy: 0.9484\n",
      "Epoch 144/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0477 - accuracy: 0.9840 - val_loss: 0.2102 - val_accuracy: 0.9449\n",
      "Epoch 145/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0395 - accuracy: 0.9840 - val_loss: 0.1340 - val_accuracy: 0.9589\n",
      "Epoch 146/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0592 - accuracy: 0.9840 - val_loss: 0.1300 - val_accuracy: 0.9587\n",
      "Epoch 147/200\n",
      "16/16 [==============================] - 4s 228ms/step - loss: 0.0591 - accuracy: 0.9780 - val_loss: 0.1169 - val_accuracy: 0.9565\n",
      "Epoch 148/200\n",
      "16/16 [==============================] - 4s 254ms/step - loss: 0.0413 - accuracy: 0.9820 - val_loss: 0.1317 - val_accuracy: 0.9579\n",
      "Epoch 149/200\n",
      "16/16 [==============================] - 4s 234ms/step - loss: 0.0716 - accuracy: 0.9660 - val_loss: 0.1527 - val_accuracy: 0.9456\n",
      "Epoch 150/200\n",
      "16/16 [==============================] - 3s 212ms/step - loss: 0.0771 - accuracy: 0.9740 - val_loss: 0.1124 - val_accuracy: 0.9560\n",
      "Epoch 151/200\n",
      "16/16 [==============================] - 4s 242ms/step - loss: 0.0455 - accuracy: 0.9760 - val_loss: 0.1158 - val_accuracy: 0.9615\n",
      "Epoch 152/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0528 - accuracy: 0.9820 - val_loss: 0.1569 - val_accuracy: 0.9518\n",
      "Epoch 153/200\n",
      "16/16 [==============================] - 3s 207ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.1201 - val_accuracy: 0.9532\n",
      "Epoch 154/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0473 - accuracy: 0.9840 - val_loss: 0.1085 - val_accuracy: 0.9598\n",
      "Epoch 155/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0430 - accuracy: 0.9820 - val_loss: 0.1092 - val_accuracy: 0.9644\n",
      "Epoch 156/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0459 - accuracy: 0.9900 - val_loss: 0.1270 - val_accuracy: 0.9591\n",
      "Epoch 157/200\n",
      "16/16 [==============================] - 4s 232ms/step - loss: 0.0406 - accuracy: 0.9820 - val_loss: 0.1431 - val_accuracy: 0.9596\n",
      "Epoch 158/200\n",
      "16/16 [==============================] - 4s 229ms/step - loss: 0.0440 - accuracy: 0.9840 - val_loss: 0.1402 - val_accuracy: 0.9553\n",
      "Epoch 159/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0584 - accuracy: 0.9740 - val_loss: 0.1299 - val_accuracy: 0.9598\n",
      "Epoch 160/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.0581 - accuracy: 0.9800 - val_loss: 0.1308 - val_accuracy: 0.9546\n",
      "Epoch 161/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0419 - accuracy: 0.9860 - val_loss: 0.1379 - val_accuracy: 0.9567\n",
      "Epoch 162/200\n",
      "16/16 [==============================] - 4s 250ms/step - loss: 0.0843 - accuracy: 0.9780 - val_loss: 0.1855 - val_accuracy: 0.9399\n",
      "Epoch 163/200\n",
      "16/16 [==============================] - 4s 248ms/step - loss: 0.0599 - accuracy: 0.9820 - val_loss: 0.1116 - val_accuracy: 0.9565\n",
      "Epoch 164/200\n",
      "16/16 [==============================] - 4s 221ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.1177 - val_accuracy: 0.9556\n",
      "Epoch 165/200\n",
      "16/16 [==============================] - 4s 241ms/step - loss: 0.1006 - accuracy: 0.9600 - val_loss: 0.1090 - val_accuracy: 0.9587\n",
      "Epoch 166/200\n",
      "16/16 [==============================] - 4s 220ms/step - loss: 0.0441 - accuracy: 0.9840 - val_loss: 0.1230 - val_accuracy: 0.9598\n",
      "Epoch 167/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0407 - accuracy: 0.9820 - val_loss: 0.1144 - val_accuracy: 0.9613\n",
      "Epoch 168/200\n",
      "16/16 [==============================] - 4s 235ms/step - loss: 0.0680 - accuracy: 0.9720 - val_loss: 0.1038 - val_accuracy: 0.9589\n",
      "Epoch 169/200\n",
      "16/16 [==============================] - 4s 245ms/step - loss: 0.0492 - accuracy: 0.9840 - val_loss: 0.1089 - val_accuracy: 0.9608\n",
      "Epoch 170/200\n",
      "16/16 [==============================] - 4s 244ms/step - loss: 0.0568 - accuracy: 0.9720 - val_loss: 0.1583 - val_accuracy: 0.9565\n",
      "Epoch 171/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.0408 - accuracy: 0.9840 - val_loss: 0.1095 - val_accuracy: 0.9608\n",
      "Epoch 172/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0513 - accuracy: 0.9800 - val_loss: 0.1110 - val_accuracy: 0.9606\n",
      "Epoch 173/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0517 - accuracy: 0.9840 - val_loss: 0.1401 - val_accuracy: 0.9584\n",
      "Epoch 174/200\n",
      "16/16 [==============================] - 4s 236ms/step - loss: 0.0555 - accuracy: 0.9860 - val_loss: 0.1584 - val_accuracy: 0.9472\n",
      "Epoch 175/200\n",
      "16/16 [==============================] - 3s 213ms/step - loss: 0.0549 - accuracy: 0.9820 - val_loss: 0.1234 - val_accuracy: 0.9539\n",
      "Epoch 176/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0441 - accuracy: 0.9880 - val_loss: 0.1227 - val_accuracy: 0.9587\n",
      "Epoch 177/200\n",
      "16/16 [==============================] - 3s 214ms/step - loss: 0.0453 - accuracy: 0.9840 - val_loss: 0.1159 - val_accuracy: 0.9587\n",
      "Epoch 178/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0324 - accuracy: 0.9880 - val_loss: 0.1304 - val_accuracy: 0.9572\n",
      "Epoch 179/200\n",
      "16/16 [==============================] - 4s 233ms/step - loss: 0.0243 - accuracy: 0.9900 - val_loss: 0.1459 - val_accuracy: 0.9632\n",
      "Epoch 180/200\n",
      "16/16 [==============================] - 4s 240ms/step - loss: 0.0504 - accuracy: 0.9880 - val_loss: 0.1294 - val_accuracy: 0.9610\n",
      "Epoch 181/200\n",
      "16/16 [==============================] - 4s 227ms/step - loss: 0.0426 - accuracy: 0.9840 - val_loss: 0.1542 - val_accuracy: 0.9427\n",
      "Epoch 182/200\n",
      "16/16 [==============================] - 4s 230ms/step - loss: 0.0368 - accuracy: 0.9860 - val_loss: 0.1538 - val_accuracy: 0.9591\n",
      "Epoch 183/200\n",
      "16/16 [==============================] - 3s 220ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.1767 - val_accuracy: 0.9606\n",
      "Epoch 184/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0484 - accuracy: 0.9840 - val_loss: 0.1439 - val_accuracy: 0.9579\n",
      "Epoch 185/200\n",
      "16/16 [==============================] - 4s 256ms/step - loss: 0.0417 - accuracy: 0.9800 - val_loss: 0.1218 - val_accuracy: 0.9575\n",
      "Epoch 186/200\n",
      "16/16 [==============================] - 4s 224ms/step - loss: 0.0298 - accuracy: 0.9880 - val_loss: 0.1414 - val_accuracy: 0.9608\n",
      "Epoch 187/200\n",
      "16/16 [==============================] - 4s 231ms/step - loss: 0.0225 - accuracy: 0.9940 - val_loss: 0.1747 - val_accuracy: 0.9520\n",
      "Epoch 188/200\n",
      "16/16 [==============================] - 4s 222ms/step - loss: 0.0350 - accuracy: 0.9860 - val_loss: 0.1524 - val_accuracy: 0.9582\n",
      "Epoch 189/200\n",
      "16/16 [==============================] - 4s 223ms/step - loss: 0.0290 - accuracy: 0.9880 - val_loss: 0.1808 - val_accuracy: 0.9575\n",
      "Epoch 190/200\n",
      "16/16 [==============================] - 4s 239ms/step - loss: 0.0465 - accuracy: 0.9860 - val_loss: 0.1494 - val_accuracy: 0.9579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8e860aad0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3 = create_cnn3_model()\n",
    "cnn3.fit(x_tensor_train, x_train_labels, epochs=200,\n",
    "                    validation_data=(x_tensor_valid, x_valid_labels),\n",
    "          callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=35)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 5ms/step - loss: 0.1172 - accuracy: 0.9676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1171889528632164, 0.9675572514533997]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3.evaluate(test_tensor, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentare il numero di livelli dense e il numero di unità di tali livelli non sembra aver portato miglioramenti.\n",
    "\n",
    "Il modello migliore è cnn2. Anche cnn3 ha delle sufficienti prestazioni, tuttavia si sceglie come modello finale la rete neurale cnn2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_41 (Conv2D)          (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " max_pooling2d_34 (MaxPoolin  (None, 14, 14, 6)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 14, 14, 6)         0         \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 14, 14, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_35 (MaxPoolin  (None, 7, 7, 16)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 7, 7, 16)          0         \n",
      "                                                                 \n",
      " conv2d_43 (Conv2D)          (None, 7, 7, 30)          12030     \n",
      "                                                                 \n",
      " max_pooling2d_36 (MaxPoolin  (None, 6, 6, 30)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 6, 6, 30)          0         \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 6, 6, 50)          37550     \n",
      "                                                                 \n",
      " max_pooling2d_37 (MaxPoolin  (None, 3, 3, 50)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 3, 3, 50)          0         \n",
      "                                                                 \n",
      " conv2d_45 (Conv2D)          (None, 1, 1, 80)          36080     \n",
      "                                                                 \n",
      " flatten_16 (Flatten)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 120)               9720      \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 10)                850       \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,977\n",
      "Trainable params: 108,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interessante è il fatto che la rete cnn2 abbia meno della metà dei parametri addestrabili rispetto alla rete fnn ottenuta ad inizio notebook. Questo dimostra il vantaggio delle reti convoluzionali su task in cui i dati sono immagini. \n",
    "\n",
    "Con tale rete si genera una confusion matrix e si misurano i valori di precision e recall.\n",
    "\n",
    "Nel codice sottostante si è applicata la threshold 0.5 per le predizioni. Dalla documentazione si evince che sia lo stesso valore di threshold che applica la funzione di attivazione sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAIhCAYAAAAimCCiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABA2ElEQVR4nO3de1hVZd7H/88GYaMIJBCnxPMhFUzFE0zmGaU8ZaVlNWpkmYeGUctRn0maStTfJGYH7WBiWqFT4VgZTzoeGlMbJJ3UzOmApb+BSPOQiIC4fn/0cz/tbjW2st3ofr+ea12Xe617r/3d+7pm5vt87nvd2CzLsgQAAAD8go+nCwAAAEDNQ5MIAAAAA00iAAAADDSJAAAAMNAkAgAAwECTCAAAAANNIgAAAAw0iQAAADDQJAIAAMBAkwhcAT777DONHj1ajRs3VkBAgOrWrasOHTpo7ty5+vHHH9362Tt27FD37t0VEhIim82m+fPnV/tn2Gw2paenV/t9f0tWVpZsNptsNps2btxoXLcsS82aNZPNZlOPHj0u6jNeeOEFZWVlufSejRs3nrcmALhcanm6AAAX9vLLL2vcuHFq2bKlHnnkEbVu3VoVFRXavn27Fi1apK1btyonJ8dtn3/fffeppKRE2dnZqlevnho1alTtn7F161bVr1+/2u9bVUFBQVq8eLHRCG7atElff/21goKCLvreL7zwgsLDwzVq1Kgqv6dDhw7aunWrWrdufdGfCwCXiiYRqMG2bt2qhx56SH379tWqVatkt9sd1/r27avJkycrNzfXrTXs3r1bY8aMUUpKits+o2vXrm67d1UMHz5cr7/+up5//nkFBwc7zi9evFiJiYk6fvz4ZamjoqJCNptNwcHBHv9NAIDpZqAGmzVrlmw2m1566SWnBvEsf39/DRo0yPH6zJkzmjt3rq6//nrZ7XZFRETo97//vQ4ePOj0vh49eiguLk55eXnq1q2b6tSpoyZNmmj27Nk6c+aMpP+bij19+rQWLlzomJaVpPT0dMe/f+nse/bv3+84t379evXo0UNhYWGqXbu2GjRooNtuu00nT550jDnXdPPu3bs1ePBg1atXTwEBAWrXrp2WLl3qNObstOybb76pGTNmKCYmRsHBwerTp4/27dtXtR9Z0l133SVJevPNNx3njh07prffflv33XffOd/z+OOPq0uXLgoNDVVwcLA6dOigxYsXy7Isx5hGjRppz5492rRpk+P3O5vEnq192bJlmjx5sq677jrZ7XZ99dVXxnTzoUOHFBsbq6SkJFVUVDju//nnnyswMFD33ntvlb8rAFQVTSJQQ1VWVmr9+vVKSEhQbGxsld7z0EMPaerUqerbt69Wr16tJ554Qrm5uUpKStKhQ4ecxhYVFenuu+/WPffco9WrVyslJUXTpk3T8uXLJUm33HKLtm7dKkm6/fbbtXXrVsfrqtq/f79uueUW+fv769VXX1Vubq5mz56twMBAlZeXn/d9+/btU1JSkvbs2aMFCxbonXfeUevWrTVq1CjNnTvXGD99+nR9++23euWVV/TSSy/pyy+/1MCBA1VZWVmlOoODg3X77bfr1VdfdZx788035ePjo+HDh5/3uz344INauXKl3nnnHQ0dOlQTJ07UE0884RiTk5OjJk2aqH379o7f79dLA6ZNm6bvvvtOixYt0rvvvquIiAjjs8LDw5Wdna28vDxNnTpVknTy5EndcccdatCggRYtWlSl7wkALrEA1EhFRUWWJOvOO++s0vi9e/dakqxx48Y5nf/kk08sSdb06dMd57p3725Jsj755BOnsa1bt7b69evndE6SNX78eKdzM2fOtM71Xx9LliyxJFkFBQWWZVnWW2+9ZUmydu7cecHaJVkzZ850vL7zzjstu91ufffdd07jUlJSrDp16lhHjx61LMuyNmzYYEmybr75ZqdxK1eutCRZW7duveDnnq03Ly/Pca/du3dblmVZnTp1skaNGmVZlmW1adPG6t69+3nvU1lZaVVUVFh/+ctfrLCwMOvMmTOOa+d779nPu+mmm857bcOGDU7n58yZY0mycnJyrJEjR1q1a9e2Pvvsswt+RwC4WCSJwFViw4YNkmQ8ING5c2e1atVK//jHP5zOR0VFqXPnzk7n2rZtq2+//bbaamrXrp38/f31wAMPaOnSpfrmm2+q9L7169erd+/eRoI6atQonTx50kg0fznlLv38PSS59F26d++upk2b6tVXX9WuXbuUl5d33qnmszX26dNHISEh8vX1lZ+fnx577DEdPnxYxcXFVf7c2267rcpjH3nkEd1yyy266667tHTpUj377LOKj4+v8vsBwBU0iUANFR4erjp16qigoKBK4w8fPixJio6ONq7FxMQ4rp8VFhZmjLPb7SotLb2Ias+tadOmWrdunSIiIjR+/Hg1bdpUTZs21TPPPHPB9x0+fPi83+Ps9V/69Xc5u37Tle9is9k0evRoLV++XIsWLVKLFi3UrVu3c47917/+peTkZEk/P33+8ccfKy8vTzNmzHD5c8/1PS9U46hRo3Tq1ClFRUWxFhGAW9EkAjWUr6+vevfurfz8fOPBk3M52ygVFhYa1/773/8qPDy82moLCAiQJJWVlTmd//W6R0nq1q2b3n33XR07dkzbtm1TYmKi0tLSlJ2dfd77h4WFnfd7SKrW7/JLo0aN0qFDh7Ro0SKNHj36vOOys7Pl5+en9957T8OGDVNSUpI6dux4UZ95rgeAzqewsFDjx49Xu3btdPjwYU2ZMuWiPhMAqoImEajBpk2bJsuyNGbMmHM+6FFRUaF3331XktSrVy9Jcjx4clZeXp727t2r3r17V1tdZ5/Q/eyzz5zOn63lXHx9fdWlSxc9//zzkqRPP/30vGN79+6t9evXO5rCs1577TXVqVPHbdvDXHfddXrkkUc0cOBAjRw58rzjbDabatWqJV9fX8e50tJSLVu2zBhbXelsZWWl7rrrLtlsNn3wwQfKyMjQs88+q3feeeeS7w0A58I+iUANlpiYqIULF2rcuHFKSEjQQw89pDZt2qiiokI7duzQSy+9pLi4OA0cOFAtW7bUAw88oGeffVY+Pj5KSUnR/v379ec//1mxsbH64x//WG113XzzzQoNDVVqaqr+8pe/qFatWsrKytKBAwecxi1atEjr16/XLbfcogYNGujUqVOOJ4j79Olz3vvPnDlT7733nnr27KnHHntMoaGhev311/X+++9r7ty5CgkJqbbv8muzZ8/+zTG33HKL5s2bpxEjRuiBBx7Q4cOH9de//vWc2xTFx8crOztbK1asUJMmTRQQEHBR6whnzpypf/7zn/rwww8VFRWlyZMna9OmTUpNTVX79u3VuHFjl+8JABdCkwjUcGPGjFHnzp2VmZmpOXPmqKioSH5+fmrRooVGjBihCRMmOMYuXLhQTZs21eLFi/X8888rJCRE/fv3V0ZGxjnXIF6s4OBg5ebmKi0tTffcc4+uueYa3X///UpJSdH999/vGNeuXTt9+OGHmjlzpoqKilS3bl3FxcVp9erVjjV959KyZUtt2bJF06dP1/jx41VaWqpWrVppyZIlLv3lEnfp1auXXn31Vc2ZM0cDBw7UddddpzFjxigiIkKpqalOYx9//HEVFhZqzJgx+umnn9SwYUOnfSSrYu3atcrIyNCf//xnp0Q4KytL7du31/Dhw7V582b5+/tXx9cDAEmSzbJ+sfMrAAAAINYkAgAA4BxoEgEAAGCgSQQAAICBJhEAAAAGmkQAAAAYaBIBAABgoEkEAACA4arcTHv9F4c9XQIAN0lqVn2bggOoWQI82JXUbj/htwddpNIdz7nt3u5EkggAAADDVZkkAgAAuMRGbvZrNIkAAAA2m6crqHFomwEAAGAgSQQAAGC62cAvAgAAAANJIgAAAGsSDSSJAAAAMJAkAgAAsCbRwC8CAAAAA0kiAAAAaxINNIkAAABMNxv4RQAAAGAgSQQAAGC62UCSCAAAAANJIgAAAGsSDfwiAAAAMJAkAgAAsCbRQJIIAAAAA0kiAAAAaxINNIkAAABMNxtomwEAAGAgSQQAAGC62cAvAgAAAANJIgAAAEmigV8EAAAABpJEAAAAH55u/jWSRAAAABhIEgEAAFiTaKBJBAAAYDNtA20zAAAADCSJAAAATDcb+EUAAABgIEkEAABgTaKBJBEAAAAGkkQAAADWJBr4RQAAAGAgSQQAAGBNooEmEQAAgOlmA78IAAAADCSJAAAATDcbSBIBAABgIEkEAABgTaKBXwQAAAAGkkQAAADWJBpIEgEAAGAgSQQAAGBNooEmEQAAgCbRwC8CAABQQyxcuFBt27ZVcHCwgoODlZiYqA8++MBxfdSoUbLZbE5H165dne5RVlamiRMnKjw8XIGBgRo0aJAOHjzoci00iQAAADab+w4X1K9fX7Nnz9b27du1fft29erVS4MHD9aePXscY/r376/CwkLHsWbNGqd7pKWlKScnR9nZ2dq8ebNOnDihAQMGqLKy0qVamG4GAACoIQYOHOj0+qmnntLChQu1bds2tWnTRpJkt9sVFRV1zvcfO3ZMixcv1rJly9SnTx9J0vLlyxUbG6t169apX79+Va6FJBEAAMDm47ajrKxMx48fdzrKysp+s6TKykplZ2erpKREiYmJjvMbN25URESEWrRooTFjxqi4uNhxLT8/XxUVFUpOTnaci4mJUVxcnLZs2eLST0KTCAAA4EYZGRkKCQlxOjIyMs47fteuXapbt67sdrvGjh2rnJwctW7dWpKUkpKi119/XevXr9fTTz+tvLw89erVy9F0FhUVyd/fX/Xq1XO6Z2RkpIqKilyqm+lmAAAAN26mPW3aNE2aNMnpnN1uP+/4li1baufOnTp69KjefvttjRw5Ups2bVLr1q01fPhwx7i4uDh17NhRDRs21Pvvv6+hQ4ee956WZcnm4nekSQQAAHAju91+wabw1/z9/dWsWTNJUseOHZWXl6dnnnlGL774ojE2OjpaDRs21JdffilJioqKUnl5uY4cOeKUJhYXFyspKcmlupluBgAAcOOaxEtlWdZ51zAePnxYBw4cUHR0tCQpISFBfn5+Wrt2rWNMYWGhdu/e7XKTSJIIAABQQ/528/Tp05WSkqLY2Fj99NNPys7O1saNG5Wbm6sTJ04oPT1dt912m6Kjo7V//35Nnz5d4eHhuvXWWyVJISEhSk1N1eTJkxUWFqbQ0FBNmTJF8fHxjqedq4omEQAAoIb4/vvvde+996qwsFAhISFq27atcnNz1bdvX5WWlmrXrl167bXXdPToUUVHR6tnz55asWKFgoKCHPfIzMxUrVq1NGzYMJWWlqp3797KysqSr6+vS7XYLMuyqvsLetr6Lw57ugQAbpLULMzTJQBwkwAPRld1bnvVbfc++fZ9bru3O7EmEQAAAAammwEAgNdzdXsYb0CSCAAAAANJIgAAAEGigSQRAAAABpJEAADg9ViTaKJJBAAAXo8m0cR0MwAAAAwkiQAAwOuRJJpIEgEAAGAgSQQAAF6PJNFEkggAAAADSSIAAABBooEkEQAAAAaSRAAA4PVYk2giSQQAAICBJBEAAHg9kkQTTSIAAPB6NIkmppsBAABgIEkEAABejyTRRJIIAAAAA0kiAAAAQaKBJBEAAAAGkkQAAOD1WJNoIkkEAACAgSQRAAB4PZJEE00iAADwejSJJqabAQAAYCBJBAAAIEg0kCQCAADAQJIIAAC8HmsSTSSJAAAAMJAkAgAAr0eSaCJJBAAAgIEkEQAAeD2SRBNNIgAA8Ho0iSammwEAAGAgSQQAACBINJAkAgAAwECSCAAAvB5rEk0kiQAAADCQJAIAAK9HkmgiSQQAAICBJBEAAHg9kkQTTSIAAAA9ooHpZgAAABhIEgEAgNdjutlEkggAAAADSSIAAPB6JIkmkkQAAIAaYuHChWrbtq2Cg4MVHBysxMREffDBB47rlmUpPT1dMTExql27tnr06KE9e/Y43aOsrEwTJ05UeHi4AgMDNWjQIB08eNDlWmgSUSN9uWeHXnjyEf1p1CA9NDhJO7dtcrq+Y+tGLZiZpin3pOihwUk68M1/znsvy7L07OOTznkfADXT999/r2lTp+impC7qknCDhg0drM/37PZ0WbiK2Ww2tx2uqF+/vmbPnq3t27dr+/bt6tWrlwYPHuxoBOfOnat58+bpueeeU15enqKiotS3b1/99NNPjnukpaUpJydH2dnZ2rx5s06cOKEBAwaosrLSpVpoElEjlZ06pesaNdPwByed83r5qVI1bdVWQ37/0G/ea/3qFUwjAFeQ48eOadQ9d6lWLT89v+hlvbP6fU1+9E8KCgr2dGmA2w0cOFA333yzWrRooRYtWuipp55S3bp1tW3bNlmWpfnz52vGjBkaOnSo4uLitHTpUp08eVJvvPGGJOnYsWNavHixnn76afXp00ft27fX8uXLtWvXLq1bt86lWliTiBopLiFRcQmJ573epWeKJOnw94UXvM/Bgi/1j79na+rTi/WnUQOrtUYA7vHq4pcVGRWlJ57KcJy77rr6HqwI3sCdYUJZWZnKysqcztntdtnt9gu+r7KyUn/7299UUlKixMREFRQUqKioSMnJyU736d69u7Zs2aIHH3xQ+fn5qqiocBoTExOjuLg4bdmyRf369aty3R5NEg8ePKgZM2aoZ8+eatWqlVq3bq2ePXtqxowZOnDggCdLw1WgvOyUFv91poY/OEkh9cI8XQ6AKtq0Yb3atInTlD8+rB7dEjXstiF6+28rPV0WrnY29x0ZGRkKCQlxOjIyMnQ+u3btUt26dWW32zV27Fjl5OSodevWKioqkiRFRkY6jY+MjHRcKyoqkr+/v+rVq3feMVXlsSRx8+bNSklJUWxsrJKTk5WcnCzLslRcXKxVq1bp2Wef1QcffKDf/e53F7zPubrz8vIy+ftfuDvH1e9vi59Rk+vjdUOXmzxdCgAXHDx4QCtXvKl7R45W6gNjtXvXZ5qT8aT8/f01cPAQT5cHuGzatGmaNMl5+dSFUsSWLVtq586dOnr0qN5++22NHDlSmzb935r6X6eelmX9ZhJalTG/5rEm8Y9//KPuv/9+ZWZmnvd6Wlqa8vLyLnifjIwMPf74407nfj/+EY2cMLXaasWV59+f/FP7PsvX9MwsT5cCwEVnzlhqExenh9N+/h/VVq1a6+uvvtLKFW/SJMJt3DndXJWp5V/y9/dXs2bNJEkdO3ZUXl6ennnmGU2d+nNvU1RUpOjoaMf44uJiR7oYFRWl8vJyHTlyxClNLC4uVlJSkkt1e2y6effu3Ro7dux5rz/44IPavfu3n2SbNm2ajh075nTc9UBaNVaKK9G+Xfk6VPT/avKIfhp/azeNv7WbJOmlOTM0b8Z4D1cH4EKuvfZaNWna1OlckyZNVFj4Xw9VBHiWZVkqKytT48aNFRUVpbVr1zqulZeXa9OmTY4GMCEhQX5+fk5jCgsLtXv3bpebRI8lidHR0dqyZYtatmx5zutbt2516pLP51zdub9/RbXUiCtXv9vu1e/6Oj+o8uTD9+r2+x5W2843eqgqAFXRrn0H7S8ocDr37f79iom5zkMVwRvUlF0wpk+f7liO99NPPyk7O1sbN25Ubm6ubDab0tLSNGvWLDVv3lzNmzfXrFmzVKdOHY0YMUKSFBISotTUVE2ePFlhYWEKDQ3VlClTFB8frz59+rhUi8eaxClTpmjs2LHKz89X3759FRkZKZvNpqKiIq1du1avvPKK5s+f76ny4GGnSk/qh8L/2/jz8PeFOvDNfxQYFKzQa6NU8tNx/fhDkY79eEiS9P3/+50kKbhemEJ+cfxa6LWRCo+MuTxfAsBFuef3IzXynrv0ykuLlNwvRbt3faa33lqpx9L/4unSALf7/vvvde+996qwsFAhISFq27atcnNz1bdvX0nSo48+qtLSUo0bN05HjhxRly5d9OGHHyooKMhxj8zMTNWqVUvDhg1TaWmpevfuraysLPn6+rpUi82yLKtav50LVqxYoczMTOXn5zs2ePT19VVCQoImTZqkYcOGXdR9139xuDrLhAf8Z9enyvyfCcb5rr1u1sg//I+2/uN9vbbgKeP6LXfepwF33X/Oez40OEkPTstQu67dq71eXD5JzXhS3Rts2rhBC+bP03ff7td19evr3t+P1m13XNz/JuDKEeDBjfmaTfngtwddpK/+muK2e7uTR5vEsyoqKnTo0M+JUHh4uPz8/C7pfjSJwNWLJhG4etEk1iw1YjNtPz+/Kq0/BAAAcIeasiaxJqkRTSIAAIAn0SOa+NvNAAAAMJAkAgAAr8d0s4kkEQAAAAaSRAAA4PUIEk0kiQAAADCQJAIAAK/n40OU+GskiQAAADCQJAIAAK/HmkQTTSIAAPB6bIFjYroZAAAABpJEAADg9QgSTSSJAAAAMJAkAgAAr8eaRBNJIgAAAAwkiQAAwOuRJJpIEgEAAGAgSQQAAF6PINFEkwgAALwe080mppsBAABgIEkEAABejyDRRJIIAAAAA0kiAADweqxJNJEkAgAAwECSCAAAvB5BookkEQAAAAaSRAAA4PVYk2giSQQAAICBJBEAAHg9gkQTTSIAAPB6TDebmG4GAACAgSQRAAB4PYJEE0kiAAAADCSJAADA67Em0USSCAAAAANJIgAA8HoEiSaSRAAAABhIEgEAgNdjTaKJJhEAAHg9ekQT080AAAAwkCQCAACvx3SziSQRAAAABpJEAADg9UgSTSSJAAAAMJAkAgAAr0eQaCJJBAAAgIEmEQAAeD2bzea2wxUZGRnq1KmTgoKCFBERoSFDhmjfvn1OY0aNGmV8RteuXZ3GlJWVaeLEiQoPD1dgYKAGDRqkgwcPulQLTSIAAPB6Npv7Dlds2rRJ48eP17Zt27R27VqdPn1aycnJKikpcRrXv39/FRYWOo41a9Y4XU9LS1NOTo6ys7O1efNmnThxQgMGDFBlZWWVa2FNIgAAQA2Rm5vr9HrJkiWKiIhQfn6+brrpJsd5u92uqKioc97j2LFjWrx4sZYtW6Y+ffpIkpYvX67Y2FitW7dO/fr1q1ItJIkAAMDruXO6uaysTMePH3c6ysrKqlTXsWPHJEmhoaFO5zdu3KiIiAi1aNFCY8aMUXFxseNafn6+KioqlJyc7DgXExOjuLg4bdmypcq/CU0iAACAG2VkZCgkJMTpyMjI+M33WZalSZMm6cYbb1RcXJzjfEpKil5//XWtX79eTz/9tPLy8tSrVy9H41lUVCR/f3/Vq1fP6X6RkZEqKiqqct1MNwMAAK/nzi1wpk2bpkmTJjmds9vtv/m+CRMm6LPPPtPmzZudzg8fPtzx77i4OHXs2FENGzbU+++/r6FDh573fpZlufQgDU0iAACAG9nt9io1hb80ceJErV69Wh999JHq169/wbHR0dFq2LChvvzyS0lSVFSUysvLdeTIEac0sbi4WElJSVWugelmAADg9XxsNrcdrrAsSxMmTNA777yj9evXq3Hjxr/5nsOHD+vAgQOKjo6WJCUkJMjPz09r1651jCksLNTu3btdahJJEgEAAGqI8ePH64033tDf//53BQUFOdYQhoSEqHbt2jpx4oTS09N12223KTo6Wvv379f06dMVHh6uW2+91TE2NTVVkydPVlhYmEJDQzVlyhTFx8c7nnauCppEAADg9WrKn+VbuHChJKlHjx5O55csWaJRo0bJ19dXu3bt0muvvaajR48qOjpaPXv21IoVKxQUFOQYn5mZqVq1amnYsGEqLS1V7969lZWVJV9f3yrXYrMsy6qWb1WDrP/isKdLAOAmSc3CPF0CADcJ8GB01e+FT9x27/8d18Vt93Yn1iQCAADAwHQzAADwej41ZLq5JiFJBAAAgIEkEQAAeD1XNpn2FiSJAAAAMJAkAgAAr0eQaCJJBAAAgIEkEQAAeD2biBJ/jSYRAAB4PbbAMTHdDAAAAANJIgAA8HpsgWMiSQQAAICBJBEAAHg9gkQTSSIAAAAMJIkAAMDr+RAlGkgSAQAAYLjkJrGyslI7d+7UkSNHqqMeAACAy85mc99xpXK5SUxLS9PixYsl/dwgdu/eXR06dFBsbKw2btxY3fUBAAC4nc1mc9txpXK5SXzrrbd0ww03SJLeffddFRQU6IsvvlBaWppmzJhR7QUCAADg8nO5STx06JCioqIkSWvWrNEdd9yhFi1aKDU1Vbt27ar2AgEAANyN6WaTy01iZGSkPv/8c1VWVio3N1d9+vSRJJ08eVK+vr7VXiAAAAAuP5e3wBk9erSGDRum6Oho2Ww29e3bV5L0ySef6Prrr6/2AgEAANyNLXBMLjeJ6enpiouL04EDB3THHXfIbrdLknx9ffWnP/2p2gsEAADA5XdRm2nffvvtxrmRI0decjEAAACeQI5oqlKTuGDBgirf8OGHH77oYgAAAFAzVKlJzMzMrNLNbDYbTSIAALjiXMn7GbpLlZrEgoICd9cBAADgMT70iIaL/rN85eXl2rdvn06fPl2d9QAAAKAGcLlJPHnypFJTU1WnTh21adNG3333naSf1yLOnj272gsEAABwN/4sn8nlJnHatGn697//rY0bNyogIMBxvk+fPlqxYkW1FgcAAADPcHkLnFWrVmnFihXq2rWrU3fcunVrff3119VaHAAAwOVwBQd+buNykvjDDz8oIiLCOF9SUnJFR6oAAAD4Py43iZ06ddL777/veH22MXz55ZeVmJhYfZUBAABcJqxJNLk83ZyRkaH+/fvr888/1+nTp/XMM89oz5492rp1qzZt2uSOGgEAAHCZuZwkJiUl6eOPP9bJkyfVtGlTffjhh4qMjNTWrVuVkJDgjhoBAADcysfmvuNKdVF/uzk+Pl5Lly6t7loAAAA84kqeFnaXi2oSKysrlZOTo71798pms6lVq1YaPHiwatW6qNsBAACghnG5q9u9e7cGDx6soqIitWzZUpL0n//8R9dee61Wr16t+Pj4ai8SAADAncgRTS6vSbz//vvVpk0bHTx4UJ9++qk+/fRTHThwQG3bttUDDzzgjhoBAABwmbmcJP773//W9u3bVa9ePce5evXq6amnnlKnTp2qtTgAAIDLwYc1iQaXk8SWLVvq+++/N84XFxerWbNm1VIUAAAAPKtKSeLx48cd/541a5Yefvhhpaenq2vXrpKkbdu26S9/+YvmzJnjnioBAADciCDRVKUm8ZprrnF6NNyyLA0bNsxxzrIsSdLAgQNVWVnphjIBAABwOVWpSdywYYO76wAAAPAY9kk0ValJ7N69u7vrAAAAQA1y0btfnzx5Ut99953Ky8udzrdt2/aSiwIAALicCBJNLjeJP/zwg0aPHq0PPvjgnNdZkwgAAK40bIFjcnkLnLS0NB05ckTbtm1T7dq1lZubq6VLl6p58+ZavXq1O2oEAADAZeZyk7h+/XplZmaqU6dO8vHxUcOGDXXPPfdo7ty5ysjIcEeNAAAAbmWzue9wRUZGhjp16qSgoCBFRERoyJAh2rdvn9MYy7KUnp6umJgY1a5dWz169NCePXucxpSVlWnixIkKDw9XYGCgBg0apIMHD7pUi8tNYklJiSIiIiRJoaGh+uGHHyRJ8fHx+vTTT129HQAAAP5/mzZt0vjx47Vt2zatXbtWp0+fVnJyskpKShxj5s6dq3nz5um5555TXl6eoqKi1LdvX/3000+OMWlpacrJyVF2drY2b96sEydOaMCAAS4tC3R5TWLLli21b98+NWrUSO3atdOLL76oRo0aadGiRYqOjnb1dgAAAB5XU7bAyc3NdXq9ZMkSRUREKD8/XzfddJMsy9L8+fM1Y8YMDR06VJK0dOlSRUZG6o033tCDDz6oY8eOafHixVq2bJn69OkjSVq+fLliY2O1bt069evXr0q1XNSaxMLCQknSzJkzlZubqwYNGmjBggWaNWuWq7cDAAC4qpWVlen48eNOR1lZWZXee+zYMUk/z95KUkFBgYqKipScnOwYY7fb1b17d23ZskWSlJ+fr4qKCqcxMTExiouLc4ypCpeTxLvvvtvx7/bt22v//v364osv1KBBA4WHh7t6O7dIbBrm6RIAuEm9ThM8XQIANynd8ZzHPtvl1MwFGRkZevzxx53OzZw5U+np6Rd8n2VZmjRpkm688UbFxcVJkoqKiiRJkZGRTmMjIyP17bffOsb4+/urXr16xpiz76+Ki94n8aw6deqoQ4cOl3obAACAq9K0adM0adIkp3N2u/033zdhwgR99tln2rx5s3Ht19PjlmX95pR5Vcb8UpWaxF9/sQuZN29elccCAADUBO5ck2i326vUFP7SxIkTtXr1an300UeqX7++43xUVJSkn9PCXz4LUlxc7EgXo6KiVF5eriNHjjilicXFxUpKSqpyDVVqEnfs2FGlm9WURZ8AAACu8KkhLYxlWZo4caJycnK0ceNGNW7c2Ol648aNFRUVpbVr16p9+/aSpPLycm3atElz5syRJCUkJMjPz09r167VsGHDJEmFhYXavXu35s6dW+VaqtQkbtiwoco3BAAAwMUZP3683njjDf39739XUFCQYw1hSEiIateuLZvNprS0NM2aNUvNmzdX8+bNNWvWLNWpU0cjRoxwjE1NTdXkyZMVFham0NBQTZkyRfHx8Y6nnaviktckAgAAXOlqSpK4cOFCSVKPHj2czi9ZskSjRo2SJD366KMqLS3VuHHjdOTIEXXp0kUffvihgoKCHOMzMzNVq1YtDRs2TKWlperdu7eysrLk6+tb5VpslmVZl/yNapjSCk9XAMBdQjvzdDNwtfLk082TVn/htnvPG3S92+7tTiSJAADA6/Fchcmd2wIBAADgCkWSCAAAvF5NWZNYk1xUkrhs2TL97ne/U0xMjGN37/nz5+vvf/97tRYHAAAAz3C5SVy4cKEmTZqkm2++WUePHlVlZaUk6ZprrtH8+fOruz4AAAC3s9ncd1ypXG4Sn332Wb388suaMWOG02PUHTt21K5du6q1OAAAgMvBx2Zz23GlcrlJLCgocOzw/Ut2u10lJSXVUhQAAAA8y+UmsXHjxtq5c6dx/oMPPlDr1q2royYAAIDLyseNx5XK5aebH3nkEY0fP16nTp2SZVn617/+pTfffFMZGRl65ZVX3FEjAAAALjOXm8TRo0fr9OnTevTRR3Xy5EmNGDFC1113nZ555hndeeed7qgRAADAra7gpYNuc1H7JI4ZM0ZjxozRoUOHdObMGUVERFR3XQAAAPCgS9pMOzw8vLrqAAAA8Jgr+Slkd3G5SWzcuPEF/77hN998c0kFAQAAwPNcbhLT0tKcXldUVGjHjh3Kzc3VI488Ul11AQAAXDYEiSaXm8Q//OEP5zz//PPPa/v27ZdcEAAAwOXG3242Vdv2PSkpKXr77ber63YAAADwoEt6cOWX3nrrLYWGhlbX7QAAAC4bHlwxudwktm/f3unBFcuyVFRUpB9++EEvvPBCtRYHAAAAz3C5SRwyZIjTax8fH1177bXq0aOHrr/++uqqCwAA4LIhSDS51CSePn1ajRo1Ur9+/RQVFeWumgAAAOBhLj24UqtWLT300EMqKytzVz0AAACXnY/NfceVyuWnm7t06aIdO3a4oxYAAADUEC6vSRw3bpwmT56sgwcPKiEhQYGBgU7X27ZtW23FAQAAXA42XcGRn5tUuUm87777NH/+fA0fPlyS9PDDDzuu2Ww2WZYlm82mysrK6q8SAADAja7kaWF3qXKTuHTpUs2ePVsFBQXurAcAAAA1QJWbRMuyJEkNGzZ0WzEAAACeQJJocunBFRubCAEAAHgFlx5cadGixW82ij/++OMlFQQAAHC5EYSZXGoSH3/8cYWEhLirFgAAANQQLjWJd955pyIiItxVCwAAgEewJtFU5TWJxLAAAADew+WnmwEAAK42ZGGmKjeJZ86ccWcdAAAAHuNDl2hw+W83AwAA4Orn8t9uBgAAuNrw4IqJJBEAAAAGkkQAAOD1WJJoIkkEAACAgSQRAAB4PR8RJf4aSSIAAAAMJIkAAMDrsSbRRJMIAAC8HlvgmJhuBgAAgIEkEQAAeD3+LJ+JJBEAAAAGkkQAAOD1CBJNJIkAAAAwkCQCAACvx5pEE0kiAAAADDSJAADA69ls7jtc9dFHH2ngwIGKiYmRzWbTqlWrnK6PGjVKNpvN6ejatavTmLKyMk2cOFHh4eEKDAzUoEGDdPDgQZfqoEkEAABez8eNh6tKSkp0ww036LnnnjvvmP79+6uwsNBxrFmzxul6WlqacnJylJ2drc2bN+vEiRMaMGCAKisrq1wHaxIBAABqkJSUFKWkpFxwjN1uV1RU1DmvHTt2TIsXL9ayZcvUp08fSdLy5csVGxurdevWqV+/flWqgyQRAAB4vV9P31bnUVZWpuPHjzsdZWVll1Tvxo0bFRERoRYtWmjMmDEqLi52XMvPz1dFRYWSk5Md52JiYhQXF6ctW7ZU+TNoEgEAANwoIyNDISEhTkdGRsZF3y8lJUWvv/661q9fr6efflp5eXnq1auXo/EsKiqSv7+/6tWr5/S+yMhIFRUVVflzmG4GAABez50b4EybNk2TJk1yOme32y/6fsOHD3f8Oy4uTh07dlTDhg31/vvva+jQoed9n2VZsrnwJA1NIgAAgBvZ7fZLagp/S3R0tBo2bKgvv/xSkhQVFaXy8nIdOXLEKU0sLi5WUlJSle/LdDMAAPB6Pjab2w53O3z4sA4cOKDo6GhJUkJCgvz8/LR27VrHmMLCQu3evdulJpEkEQAAoAY5ceKEvvrqK8frgoIC7dy5U6GhoQoNDVV6erpuu+02RUdHa//+/Zo+fbrCw8N16623SpJCQkKUmpqqyZMnKywsTKGhoZoyZYri4+MdTztXBU0iAADwejXpj/Jt375dPXv2dLw+u55x5MiRWrhwoXbt2qXXXntNR48eVXR0tHr27KkVK1YoKCjI8Z7MzEzVqlVLw4YNU2lpqXr37q2srCz5+vpWuQ6bZVlW9X2tmqG0wtMVAHCX0M4TPF0CADcp3XH+zaPd7Y1PXftrJK4Y0aG+2+7tTqxJBAAAgIHpZgAA4PVc2RrGW5AkAgAAwECSCAAAvB6pmYnfBAAAAAaSRAAA4PVYk2giSQQAAICBJBEAAHg9ckQTSSIAAAAMJIkAAMDrsSbRRJMIAAC8HlOrJn4TAAAAGEgSAQCA12O62USSCAAAAANJIgAA8HrkiCaSRAAAABhIEgEAgNdjSaKJJBEAAAAGkkQAAOD1fFiVaKBJBAAAXo/pZhPTzQAAADCQJAIAAK9nY7rZQJIIAAAAA0kiAADweqxJNJEkAgAAwECSCAAAvB5b4JhIEgEAAGAgSQQAAF6PNYkmmkQAAOD1aBJNTDcDAADAQJIIAAC8Hptpm0gSAQAAYCBJBAAAXs+HINFAkggAAAADSSIAAPB6rEk0kSQCAADAQJIIAAC8HvskmmgSAQCA12O62cR0MwAAAAwkiQAAwOuxBY6JJBEAAAAGkkQAAOD1WJNoIkkEAACAgSQRAAB4PbbAMZEkAgAAwECSCAAAvB5BookmEQAAeD0f5psNNXq6+cCBA7rvvvsuOKasrEzHjx93OsrKyi5ThQAAAFenGt0k/vjjj1q6dOkFx2RkZCgkJMTp+H/mZFymCgEAwNXA5sbDVR999JEGDhyomJgY2Ww2rVq1yum6ZVlKT09XTEyMateurR49emjPnj1OY8rKyjRx4kSFh4crMDBQgwYN0sGDB12qw6PTzatXr77g9W+++eY37zFt2jRNmjTJ6dwZH/sl1QUAAOApJSUluuGGGzR69GjddtttxvW5c+dq3rx5ysrKUosWLfTkk0+qb9++2rdvn4KCgiRJaWlpevfdd5Wdna2wsDBNnjxZAwYMUH5+vnx9fatUh82yLKtav5kLfHx8ZLPZdKESbDabKisrXbpvacWlVgagpgrtPMHTJQBwk9Idz3nss7d9fdRt9+7a9JqLfq/NZlNOTo6GDBki6ecUMSYmRmlpaZo6daqkn1PDyMhIzZkzRw8++KCOHTuma6+9VsuWLdPw4cMlSf/9738VGxurNWvWqF+/flX6bI9ON0dHR+vtt9/WmTNnznl8+umnniwPAADgklXn8xMFBQUqKipScnKy45zdblf37t21ZcsWSVJ+fr4qKiqcxsTExCguLs4xpio82iQmJCRcsBH8rZQRAACgOtjc+H/nen4iI+Pinp8oKiqSJEVGRjqdj4yMdFwrKiqSv7+/6tWrd94xVeHRNYmPPPKISkpKznu9WbNm2rBhw2WsCAAAoHqd6/kJu/3Snp+w/WrLHsuyjHO/VpUxv+TRJrFbt24XvB4YGKju3btfpmoAAIC3cuc2iXa7/ZKbwrOioqIk/ZwWRkdHO84XFxc70sWoqCiVl5fryJEjTmlicXGxkpKSqvxZNXoLHAAAgMuhJm2BcyGNGzdWVFSU1q5d6zhXXl6uTZs2ORrAhIQE+fn5OY0pLCzU7t27XWoS+YsrAAAANciJEyf01VdfOV4XFBRo586dCg0NVYMGDZSWlqZZs2apefPmat68uWbNmqU6depoxIgRkqSQkBClpqZq8uTJCgsLU2hoqKZMmaL4+Hj16dOnynXQJAIAANSgv8q3fft29ezZ0/H67HrGkSNHKisrS48++qhKS0s1btw4HTlyRF26dNGHH37o2CNRkjIzM1WrVi0NGzZMpaWl6t27t7Kysqq8R6Lk4X0S3YV9EoGrF/skAlcvT+6TmFdwzG337tQ4xG33dieSRAAA4PVsNSlKrCF4cAUAAAAGkkQAAOD13LkFzpWKJBEAAAAGkkQAAOD1CBJNNIkAAAB0iQammwEAAGAgSQQAAF6PLXBMJIkAAAAwkCQCAACvxxY4JpJEAAAAGEgSAQCA1yNINJEkAgAAwECSCAAAQJRooEkEAABejy1wTEw3AwAAwECSCAAAvB5b4JhIEgEAAGAgSQQAAF6PINFEkggAAAADSSIAAABRooEkEQAAAAaSRAAA4PXYJ9FEkggAAAADSSIAAPB67JNookkEAABejx7RxHQzAAAADCSJAAAARIkGkkQAAAAYSBIBAIDXYwscE0kiAAAADCSJAADA67EFjokkEQAAAAaSRAAA4PUIEk00iQAAAHSJBqabAQAAYCBJBAAAXo8tcEwkiQAAADCQJAIAAK/HFjgmkkQAAAAYSBIBAIDXI0g0kSQCAADAQJIIAABAlGigSQQAAF6PLXBMTDcDAADAQJIIAAC8HlvgmEgSAQAAYCBJBAAAXo8g0USSCAAAAANNIgAAgM2NhwvS09Nls9mcjqioKMd1y7KUnp6umJgY1a5dWz169NCePXsu+mtfCE0iAABADdKmTRsVFhY6jl27djmuzZ07V/PmzdNzzz2nvLw8RUVFqW/fvvrpp5+qvQ7WJAIAAK/nzn0Sy8rKVFZW5nTObrfLbrefc3ytWrWc0sOzLMvS/PnzNWPGDA0dOlSStHTpUkVGRuqNN97Qgw8+WK11kyQCAACvZ7O578jIyFBISIjTkZGRcd5avvzyS8XExKhx48a688479c0330iSCgoKVFRUpOTkZMdYu92u7t27a8uWLdX+m5AkAgAAuNG0adM0adIkp3PnSxG7dOmi1157TS1atND333+vJ598UklJSdqzZ4+KiookSZGRkU7viYyM1LffflvtddMkAgAAr+fOLXAuNLX8aykpKY5/x8fHKzExUU2bNtXSpUvVtWtXSZLtVzt/W5ZlnKsOTDcDAADUUIGBgYqPj9eXX37pWKd4NlE8q7i42EgXqwNNIgAA8HruXJN4KcrKyrR3715FR0ercePGioqK0tq1ax3Xy8vLtWnTJiUlJV3iL2BiuhkAAKCGmDJligYOHKgGDRqouLhYTz75pI4fP66RI0fKZrMpLS1Ns2bNUvPmzdW8eXPNmjVLderU0YgRI6q9FppEAACAGvKH+Q4ePKi77rpLhw4d0rXXXquuXbtq27ZtatiwoSTp0UcfVWlpqcaNG6cjR46oS5cu+vDDDxUUFFTttdgsy7Kq/a4eVlrh6QoAuEto5wmeLgGAm5TueM5jn33wSLnb7l2/nr/b7u1OJIkAAMDrueHh4CseTSIAAPB69Igmnm4GAACAgSQRAAB4PaabTSSJAAAAMJAkAgAAr2djVaKBJBEAAAAGkkQAAACCRANJIgAAAAwkiQAAwOsRJJpoEgEAgNdjCxwT080AAAAwkCQCAACvxxY4JpJEAAAAGEgSAQAACBINJIkAAAAwkCQCAACvR5BoIkkEAACAgSQRAAB4PfZJNNEkAgAAr8cWOCammwEAAGAgSQQAAF6P6WYTSSIAAAAMNIkAAAAw0CQCAADAwJpEAADg9ViTaCJJBAAAgIEkEQAAeD32STTRJAIAAK/HdLOJ6WYAAAAYSBIBAIDXI0g0kSQCAADAQJIIAABAlGggSQQAAICBJBEAAHg9tsAxkSQCAADAQJIIAAC8HvskmkgSAQAAYCBJBAAAXo8g0USTCAAAQJdoYLoZAAAABpJEAADg9dgCx0SSCAAAAANJIgAA8HpsgWMiSQQAAIDBZlmW5ekigItVVlamjIwMTZs2TXa73dPlAKhG/Ocb8CyaRFzRjh8/rpCQEB07dkzBwcGeLgdANeI/34BnMd0MAAAAA00iAAAADDSJAAAAMNAk4opmt9s1c+ZMFrUDVyH+8w14Fg+uAAAAwECSCAAAAANNIgAAAAw0iQAAADDQJAIAAMBAk4gr2gsvvKDGjRsrICBACQkJ+uc//+npkgBcoo8++kgDBw5UTEyMbDabVq1a5emSAK9Ek4gr1ooVK5SWlqYZM2Zox44d6tatm1JSUvTdd995ujQAl6CkpEQ33HCDnnvuOU+XAng1tsDBFatLly7q0KGDFi5c6DjXqlUrDRkyRBkZGR6sDEB1sdlsysnJ0ZAhQzxdCuB1SBJxRSovL1d+fr6Sk5OdzicnJ2vLli0eqgoAgKsHTSKuSIcOHVJlZaUiIyOdzkdGRqqoqMhDVQEAcPWgScQVzWazOb22LMs4BwAAXEeTiCtSeHi4fH19jdSwuLjYSBcBAIDraBJxRfL391dCQoLWrl3rdH7t2rVKSkryUFUAAFw9anm6AOBiTZo0Sffee686duyoxMREvfTSS/ruu+80duxYT5cG4BKcOHFCX331leN1QUGBdu7cqdDQUDVo0MCDlQHehS1wcEV74YUXNHfuXBUWFiouLk6ZmZm66aabPF0WgEuwceNG9ezZ0zg/cuRIZWVlXf6CAC9FkwgAAAADaxIBAABgoEkEAACAgSYRAAAABppEAAAAGGgSAQAAYKBJBAAAgIEmEQAAAAaaRAAAABhoEgFcsvT0dLVr187xetSoURoyZMhlr2P//v2y2WzauXPnecc0atRI8+fPr/I9s7KydM0111xybTabTatWrbrk+wDA5UKTCFylRo0aJZvNJpvNJj8/PzVp0kRTpkxRSUmJ2z/7mWeeqfKfT6tKYwcAuPxqeboAAO7Tv39/LVmyRBUVFfrnP/+p+++/XyUlJVq4cKExtqKiQn5+ftXyuSEhIdVyHwCA55AkAlcxu92uqKgoxcbGasSIEbr77rsdU55np4hfffVVNWnSRHa7XZZl6dixY3rggQcUERGh4OBg9erVS//+97+d7jt79mxFRkYqKChIqampOnXqlNP1X083nzlzRnPmzFGzZs1kt9vVoEEDPfXUU5Kkxo0bS5Lat28vm82mHj16ON63ZMkStWrVSgEBAbr++uv1wgsvOH3Ov/71L7Vv314BAQHq2LGjduzY4fJvNG/ePMXHxyswMFCxsbEaN26cTpw4YYxbtWqVWrRooYCAAPXt21cHDhxwuv7uu+8qISFBAQEBatKkiR5//HGdPn36nJ9ZXl6uCRMmKDo6WgEBAWrUqJEyMjJcrh0A3IkkEfAitWvXVkVFheP1V199pZUrV+rtt9+Wr6+vJOmWW25RaGio1qxZo5CQEL344ovq3bu3/vOf/yg0NFQrV67UzJkz9fzzz6tbt25atmyZFixYoCZNmpz3c6dNm6aXX35ZmZmZuvHGG1VYWKgvvvhC0s+NXufOnbVu3Tq1adNG/v7+kqSXX35ZM2fO1HPPPaf27dtrx44dGjNmjAIDAzVy5EiVlJRowIAB6tWrl5YvX66CggL94Q9/cPk38fHx0YIFC9SoUSMVFBRo3LhxevTRR50a0pMnT+qpp57S0qVL5e/vr3HjxunOO+/Uxx9/LEn63//9X91zzz1asGCBunXrpq+//loPPPCAJGnmzJnGZy5YsECrV6/WypUr1aBBAx04cMBoOgHA4ywAV6WRI0dagwcPdrz+5JNPrLCwMGvYsGGWZVnWzJkzLT8/P6u4uNgx5h//+IcVHBxsnTp1yuleTZs2tV588UXLsiwrMTHRGjt2rNP1Ll26WDfccMM5P/v48eOW3W63Xn755XPWWVBQYEmyduzY4XQ+NjbWeuONN5zOPfHEE1ZiYqJlWZb14osvWqGhoVZJSYnj+sKFC895r19q2LChlZmZed7rK1eutMLCwhyvlyxZYkmytm3b5ji3d+9eS5L1ySefWJZlWd26dbNmzZrldJ9ly5ZZ0dHRjteSrJycHMuyLGvixIlWr169rDNnzpy3DgDwNJJE4Cr23nvvqW7dujp9+rQqKio0ePBgPfvss47rDRs21LXXXut4nZ+frxMnTigsLMzpPqWlpfr6668lSXv37tXYsWOdricmJmrDhg3nrGHv3r0qKytT7969q1z3Dz/8oAMHDig1NVVjxoxxnD99+rRjvePevXt1ww03qE6dOk51uGrDhg2aNWuWPv/8cx0/flynT5/WqVOnVFJSosDAQElSrVq11LFjR8d7rr/+el1zzTXau3evOnfurPz8fOXl5Tmm0CWpsrJSp06d0smTJ51qlH6eju/bt69atmyp/v37a8CAAUpOTna5dgBwJ5pE4CrWs2dPLVy4UH5+foqJiTEeTDnbBJ115swZRUdHa+PGjca9LnYbmNq1a7v8njNnzkj6ecq5S5cuTtfOTotblnVR9fzSt99+q5tvvlljx47VE088odDQUG3evFmpqalO0/LSz1vY/NrZc2fOnNHjjz+uoUOHGmMCAgKMcx06dFBBQYE++OADrVu3TsOGDVOfPn301ltvXfJ3AoDqQpMIXMUCAwPVrFmzKo/v0KGDioqKVKtWLTVq1OicY1q1aqVt27bp97//vePctm3bznvP5s2bq3bt2vrHP/6h+++/37h+dg1iZWWl41xkZKSuu+46ffPNN7r77rvPed/WrVtr2bJlKi0tdTSiF6rjXLZv367Tp0/r6aeflo/Pz8/xrVy50hh3+vRpbd++XZ07d5Yk7du3T0ePHtX1118v6effbd++fS791sHBwRo+fLiGDx+u22+/Xf3799ePP/6o0NBQl74DALgLTSIAhz59+igxMVFDhgzRnDlz1LJlS/33v//VmjVrNGTIEHXs2FF/+MMfNHLkSHXs2FE33nijXn/9de3Zs+e8D64EBARo6tSpevTRR+Xv76/f/e53+uGHH7Rnzx6lpqYqIiJCtWvXVm5ururXr6+AgACFhIQoPT1dDz/8sIKDg5WSkqKysjJt375dR44c0aRJkzRixAjNmDFDqamp+p//+R/t379ff/3rX136vk2bNtXp06f17LPPauDAgfr444+1aNEiY5yfn58mTpyoBQsWyM/PTxMmTFDXrl0dTeNjjz2mAQMGKDY2VnfccYd8fHz02WefadeuXXryySeN+2VmZio6Olrt2rWTj4+P/va3vykqKqpaNu0GgOrCFjgAHGw2m9asWaObbrpJ9913n1q0aKE777xT+/fvV2RkpCRp+PDheuyxxzR16lQlJCTo22+/1UMPPXTB+/75z3/W5MmT9dhjj6lVq1YaPny4iouLJf283m/BggV68cUXFRMTo8GDB0uS7r//fr3yyivKyspSfHy8unfvrqysLMeWOXXr1tW7776rzz//XO3bt9eMGTM0Z84cl75vu3btNG/ePM2ZM0dxcXF6/fXXz7kVTZ06dTR16lSNGDFCiYmJql27trKzsx3X+/Xrp/fee09r165Vp06d1LVrV82bN08NGzY85+fWrVtXc+bMUceOHdWpUyft379fa9ascaSZAFAT2KzqWNgDAACAqwr/bysAAAAMNIkAAAAw0CQCAADAQJMIAAAAA00iAAAADDSJAAAAMNAkAgAAwECTCAAAAANNIgAAAAw0iQAAADDQJAIAAMDw/wEHjY3BBACS3AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9850374064837906\n",
      "Recall: 0.9777227722772277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "predictions = cnn2.predict(test_tensor)\n",
    "y_pred = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "precision = precision_score(test_labels, y_pred)\n",
    "recall = recall_score(test_labels, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Precision:\",precision)\n",
    "print(\"Recall:\",recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella ricerca dei migliori parametri molti test non sono stati riportati nel notebook. Sono stati testati molti modelli di reti neurali con numero differenti di livelli convoluzionali, in alcuni casi il numero di layer era talmente eccessivo che il modello aveva zero capacità di generalizzazione. Sono stati testati anche differenti dimensioni per i filtri, ovvero valori 3,5,7,11. I miglior valori sono stati 5 e 3. Anche il numero di filtri è stato fatto variare. Dai vari test si è notato che un numero eccessivo (come atteso) comporta overfitting. Inoltre un numero fisso come numero di filtri non è ottimale. La miglior configurazione di numero di filtri sembra essere una configurazione crescente. \n",
    "\n",
    "Dalla confusion matrix possiamo osservare come il test sia formato da molti dati della classe 1, di cui quasi la totalità è stata correttamente etichettata. Similmente per i dati della classe 0, che possiede meno dati. La disparità nella presenza delle classi nel test set dipende anche dal fatto che la classe 1 possiede il triplo di dati rispetto alla classe 0.\n",
    "\n",
    "Il valore alto della accuracy del modello cnn2 e il valore relativamente basso della loss (valori su test set) indicano che il modello implementato è sufficientemente capace di classificare i dati del nostro dataset. Anche i valori di precision e recall sono sufficientemente alti, quindi anche questi testimoniano la capacità sufficiente della rete.\n",
    "\n",
    "\n",
    "Per valutare i modelli cnn1, cnn2, cnn3 non è stata utilizzata la tecnica di cross validation per il fatto che tale tecnica richiede un tempo eccessivo.\n",
    "cnn1 richiede circa 4 secondi ad epoca per all'incirca 50 epoche, cnn2 5 secondi ad epoca per 120 epoche e cnn3 4 secondi ad epoca per 180 epoche. Quindi il tempo richiesto per addestrare tali reti è circa 25 minuti per iterazione di addestramento. Applicare la tecnica di cross validation con k fold, ad esempio k=10 come consigliato, comporterebbe un costo di training di oltre 4 ore.\n",
    "Inoltre il secondo motivo per cui non è stata utilizzata tale tecnica è il fatto che il miglior modello è cnn2, poichè estende cnn1, mentre cnn3 non migliora le prestazioni. \n",
    "\n",
    "Durante i vari test è stato cambiata la variabile \"seed\" ad inizio notebook. Tale variabile condiziona come vengono suddivisi i dati del dataset in train e test set. Quindi indirettamente variava anche il validation set. Tale effetto non è paragonabile all cross validation, assolutamente, però ha fatto variare i risultati dei training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#codice per effettuare uno pseudo cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "num_folds = 3\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "#lista per mantenere risultati\n",
    "cv_scores_cnn2 = []\n",
    "cv_scores_cnn3 = []\n",
    "\n",
    "for fold_idx, (train_indices, val_indices) in enumerate(kf.split(train_images)):\n",
    "    print(f\"Fold {fold_idx + 1}/{num_folds}\")\n",
    "    \n",
    "    #divisione in train e validation\n",
    "    x_train_fold, x_val_fold = train_images[train_indices], train_images[val_indices]\n",
    "    y_train_fold, y_val_fold = train_labels[train_indices], train_labels[val_indices]\n",
    "\n",
    "    x_tensor_train_fold = x_train_fold.reshape(-1,28,28,1)\n",
    "    x_tensor_valid_fold = x_val_fold.reshape(-1,28,28,1)\n",
    "    \n",
    "    cv_cnn2 = create_cnn2_model()\n",
    "    cv_cnn3 = create_cnn3_model()\n",
    "    \n",
    "    cv_cnn2.fit(x_tensor_train_fold, y_train_fold, \n",
    "                validation_data=(x_tensor_valid_fold, y_val_fold), \n",
    "                epochs=100, \n",
    "                callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=20)],\n",
    "                batch_size=32, verbose=0)\n",
    "    cv_cnn3.fit(x_tensor_train_fold, y_train_fold, \n",
    "                validation_data=(x_tensor_valid_fold, y_val_fold), \n",
    "                epochs=100, \n",
    "                callbacks=[keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=20)],\n",
    "                batch_size=32, verbose=0)\n",
    "    \n",
    "    val_loss2, val_acc2 = cv_cnn2.evaluate(x_tensor_valid_fold, y_val_fold, verbose=0)\n",
    "    val_loss3, val_acc3 = cv_cnn3.evaluate(x_tensor_valid_fold, y_val_fold, verbose=0)\n",
    "    cv_scores_cnn2.append(val_acc2)\n",
    "    cv_scores_cnn3.append(val_acc3)\n",
    "\n",
    "\n",
    "mean_cv_accuracy_cnn2 = np.mean(cv_scores_cnn2)\n",
    "mean_cv_accuracy_cnn3 = np.mean(cv_scores_cnn3)\n",
    "\n",
    "print(\"Mean Cross-Validation Accuracy for cnn2: \",mean_cv_accuracy_cnn2)\n",
    "print(\"Mean Cross-Validation Accuracy for cnn3: \",mean_cv_accuracy_cnn3)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMFFoeEH9usy0MDC6p3w150",
   "provenance": [
    {
     "file_id": "1G8HKhaNVlQZsc8HNx1eNwzmGiqvc4K_3",
     "timestamp": 1707507531600
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
